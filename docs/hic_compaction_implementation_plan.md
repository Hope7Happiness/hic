# HIC Context Compaction å®ç°æ–¹æ¡ˆ

## æ–‡æ¡£ä¿¡æ¯
- **åˆ›å»ºæ—¶é—´**: 2026-01-27
- **ç›®æ ‡**: ä¸ºHIC Agentæ¡†æ¶å®ç°context compactionåŠŸèƒ½
- **å‚è€ƒ**: OpenCode Compactionå®ç°åˆ†æ
- **çŠ¶æ€**: å¾…å®¡æ ¸

---

## ç›®å½•
1. [éœ€æ±‚åˆ†æ](#éœ€æ±‚åˆ†æ)
2. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
3. [å®ç°è®¡åˆ’](#å®ç°è®¡åˆ’)
4. [è¯¦ç»†è®¾è®¡](#è¯¦ç»†è®¾è®¡)
5. [æµ‹è¯•æ–¹æ¡ˆ](#æµ‹è¯•æ–¹æ¡ˆ)
6. [é£é™©ä¸ç¼“è§£](#é£é™©ä¸ç¼“è§£)

---

## éœ€æ±‚åˆ†æ

### æ ¸å¿ƒé—®é¢˜
å½“å‰HIC Agentåœ¨å¤„ç†é•¿å¯¹è¯æ—¶ï¼ŒLLMçš„chat historyä¼šä¸æ–­å¢é•¿ï¼Œæœ€ç»ˆä¼šè§¦å‘ä»¥ä¸‹é—®é¢˜ï¼š
1. **Tokené™åˆ¶é”™è¯¯**: è¶…è¿‡æ¨¡å‹çš„context windowï¼ˆå¦‚Claude: 200K, GPT-4: 128Kï¼‰
2. **APIè°ƒç”¨å¤±è´¥**: ç›´æ¥å¯¼è‡´agentå´©æºƒ
3. **æˆæœ¬å¢åŠ **: å³ä½¿ä¸è¶…é™ï¼Œè¿‡é•¿çš„contextä¹Ÿä¼šå¢åŠ APIæˆæœ¬

### åŠŸèƒ½ç›®æ ‡
å®ç°è‡ªåŠ¨åŒ–çš„context compactionæœºåˆ¶ï¼ŒåŒ…æ‹¬ï¼š
1. âœ… **è‡ªåŠ¨æ£€æµ‹**: æ£€æµ‹chat historyæ˜¯å¦æ¥è¿‘token limit
2. âœ… **è‡ªåŠ¨å‹ç¼©**: è§¦å‘compaction agentæ€»ç»“å†å²å¯¹è¯
3. âœ… **é€æ˜æ‰§è¡Œ**: å¯¹ç”¨æˆ·é€æ˜ï¼Œä¸ä¸­æ–­æ­£å¸¸æµç¨‹
4. âœ… **ä¿¡æ¯ä¿ç•™**: æ€»ç»“åº”ä¿ç•™å…³é”®ä¿¡æ¯ä»¥ç»§ç»­å¯¹è¯
5. âœ… **å¯é…ç½®**: å…è®¸ç”¨æˆ·é…ç½®æ˜¯å¦å¯ç”¨ã€è§¦å‘é˜ˆå€¼ç­‰

### éç›®æ ‡ï¼ˆæš‚ä¸å®ç°ï¼‰
- âŒ Tool output pruningï¼ˆP2ä¼˜å…ˆçº§ï¼Œæœ¬æ¬¡ä¸åšï¼‰
- âŒ æ’ä»¶ç³»ç»Ÿæ‰©å±•ç‚¹ï¼ˆæš‚æ— æ’ä»¶ç³»ç»Ÿï¼‰
- âŒ æ‰‹åŠ¨è§¦å‘compactionï¼ˆè‡ªåŠ¨å³å¯ï¼‰
- âŒ å¤šçº§å‹ç¼©ï¼ˆä¸€æ¬¡å‹ç¼©è¶³å¤Ÿï¼‰

---

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HIC Agent                                â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚   Agent      â”‚         â”‚ Compaction   â”‚                      â”‚
â”‚  â”‚   (_run)     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Detector    â”‚                      â”‚
â”‚  â”‚              â”‚         â”‚              â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚         â”‚                         â”‚                              â”‚
â”‚         â”‚                         â”‚ Token overflow detected      â”‚
â”‚         â”‚                         â–¼                              â”‚
â”‚         â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚         â”‚                â”‚ Compaction   â”‚                        â”‚
â”‚         â”‚                â”‚   Agent      â”‚                        â”‚
â”‚         â”‚                â”‚  (special)   â”‚                        â”‚
â”‚         â”‚                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚                        â”‚                               â”‚
â”‚         â”‚                        â”‚ Generate summary              â”‚
â”‚         â”‚                        â–¼                               â”‚
â”‚         â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚         â”‚                â”‚  LLM.chat()  â”‚                        â”‚
â”‚         â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ (all history)â”‚                        â”‚
â”‚         â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚                                                         â”‚
â”‚         â”‚  Compacted history = [summary message]                 â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ Continue withâ”‚                                                â”‚
â”‚  â”‚ new context  â”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

#### 1. TokenCounterï¼ˆæ–°å¢ï¼‰
**ä½ç½®**: `agent/token_counter.py`

**èŒè´£**: 
- ä¼°ç®—chat historyçš„tokenæ•°é‡
- æä¾›å¤šç§è®¡æ•°ç­–ç•¥ï¼ˆç®€å•/ç²¾ç¡®ï¼‰

#### 2. CompactionDetectorï¼ˆæ–°å¢ï¼‰
**ä½ç½®**: `agent/compaction.py`

**èŒè´£**:
- æ£€æµ‹æ˜¯å¦éœ€è¦compaction
- ç®¡ç†compactionè§¦å‘é€»è¾‘

#### 3. CompactionAgentï¼ˆæ–°å¢ï¼‰
**ä½ç½®**: `agent/compaction.py`

**èŒè´£**:
- æ‰§è¡Œcompactionï¼ˆè°ƒç”¨LLMç”Ÿæˆæ€»ç»“ï¼‰
- ç”Ÿæˆæ–°çš„compacted history

#### 4. Agentï¼ˆä¿®æ”¹ï¼‰
**ä½ç½®**: `agent/agent.py`

**ä¿®æ”¹ç‚¹**:
- åœ¨`_internal_run`å’Œ`_internal_resume`ä¸­æ£€æµ‹overflow
- è§¦å‘compactionæµç¨‹
- ç”¨compacted historyæ›¿æ¢åŸhistory

#### 5. LLMï¼ˆæ‰©å±•ï¼‰
**ä½ç½®**: `agent/llm.py`

**æ–°å¢æ–¹æ³•**:
- `count_tokens()`: è¿”å›å½“å‰historyçš„tokenæ•°
- `compact_history()`: å¯¹historyè¿›è¡Œå‹ç¼©ï¼ˆå¯é€‰ï¼Œä½œä¸ºå·¥å…·æ–¹æ³•ï¼‰

---

## å®ç°è®¡åˆ’

### é˜¶æ®µåˆ’åˆ†

#### Phase 1: åŸºç¡€è®¾æ–½ï¼ˆP0ï¼‰
**ç›®æ ‡**: å»ºç«‹tokenè®¡æ•°å’Œé…ç½®åŸºç¡€

**ä»»åŠ¡**:
1. âœ… å®ç°`TokenCounter`ç±»
   - ç®€å•ä¼°ç®—ï¼ˆchars/4ï¼‰
   - ç²¾ç¡®è®¡æ•°ï¼ˆtiktokenï¼‰
2. âœ… æ·»åŠ é…ç½®é€‰é¡¹åˆ°`agent/config.py`
3. âœ… ä¸º`LLM`æ·»åŠ `count_tokens()`æ–¹æ³•
4. âœ… ç¼–å†™å•å…ƒæµ‹è¯•

**éªŒæ”¶æ ‡å‡†**:
- èƒ½å¤Ÿå‡†ç¡®è®¡æ•°ä¸åŒLLMçš„history tokens
- é…ç½®å¯ä»¥æ­£ç¡®åŠ è½½

#### Phase 2: Compactionæ ¸å¿ƒï¼ˆP1ï¼‰
**ç›®æ ‡**: å®ç°compactionæ£€æµ‹å’Œæ‰§è¡Œ

**ä»»åŠ¡**:
1. âœ… å®ç°`CompactionDetector.should_compact()`
2. âœ… å®ç°`CompactionAgent.compact()`
3. âœ… åˆ›å»ºcompaction system prompt
4. âœ… ä¿®æ”¹`Agent._internal_run()`é›†æˆæ£€æµ‹
5. âœ… ä¿®æ”¹`Agent._internal_resume()`é›†æˆæ£€æµ‹
6. âœ… ç¼–å†™é›†æˆæµ‹è¯•

**éªŒæ”¶æ ‡å‡†**:
- Overflowèƒ½å¤Ÿæ­£ç¡®æ£€æµ‹
- Compactionèƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆæ€»ç»“
- Agentèƒ½å¤Ÿç”¨compacted historyç»§ç»­è¿è¡Œ

#### Phase 3: ä¼˜åŒ–ä¸å®Œå–„ï¼ˆP2ï¼‰
**ç›®æ ‡**: æå‡ç”¨æˆ·ä½“éªŒå’Œæ€§èƒ½

**ä»»åŠ¡**:
1. âœ… æ·»åŠ compactionæ—¥å¿—
2. âœ… ä¼˜åŒ–compaction prompt
3. âœ… æ·»åŠ ä¿æŠ¤ç­–ç•¥ï¼ˆä¿ç•™æœ€è¿‘Næ¡æ¶ˆæ¯ï¼‰
4. âœ… æ·»åŠ compactionå¤±è´¥å¤„ç†
5. âœ… æ€§èƒ½æµ‹è¯•

**éªŒæ”¶æ ‡å‡†**:
- Compactionè¿‡ç¨‹æœ‰æ¸…æ™°æ—¥å¿—
- å¤±è´¥èƒ½å¤Ÿä¼˜é›…é™çº§
- æ€§èƒ½æ»¡è¶³è¦æ±‚

---

## è¯¦ç»†è®¾è®¡

### 1. Tokenè®¡æ•°æ¨¡å—

#### æ–‡ä»¶: `agent/token_counter.py`

```python
"""
Token counting utilities for context compaction.

Provides both simple (heuristic) and accurate (tiktoken) token counting.
"""

from typing import List, Dict, Optional
from abc import ABC, abstractmethod


class TokenCounter(ABC):
    """Abstract base class for token counting."""
    
    @abstractmethod
    def count(self, text: str) -> int:
        """Count tokens in a string."""
        pass
    
    @abstractmethod
    def count_messages(self, messages: List[Dict[str, str]]) -> int:
        """Count tokens in a list of messages."""
        pass


class SimpleTokenCounter(TokenCounter):
    """
    Simple heuristic-based token counter.
    
    Uses the approximation: 1 token â‰ˆ 4 characters.
    Fast but less accurate.
    """
    
    CHARS_PER_TOKEN = 4
    
    def count(self, text: str) -> int:
        """Count tokens using character-based heuristic."""
        return max(0, len(text) // self.CHARS_PER_TOKEN)
    
    def count_messages(self, messages: List[Dict[str, str]]) -> int:
        """Count tokens in message list."""
        total = 0
        for msg in messages:
            # Count role
            total += self.count(msg.get("role", ""))
            # Count content
            total += self.count(msg.get("content", ""))
            # Add overhead for message structure (estimate)
            total += 4
        return total


class TiktokenCounter(TokenCounter):
    """
    Accurate token counter using tiktoken library.
    
    Slower but more accurate, especially for non-English text.
    """
    
    def __init__(self, model: str = "gpt-4"):
        """
        Initialize with a specific model encoding.
        
        Args:
            model: Model name (e.g., "gpt-4", "gpt-3.5-turbo")
        """
        try:
            import tiktoken
            self.encoding = tiktoken.encoding_for_model(model)
            self.available = True
        except ImportError:
            print("Warning: tiktoken not installed. Falling back to simple counter.")
            self.available = False
            self.simple_counter = SimpleTokenCounter()
        except Exception as e:
            print(f"Warning: Failed to load tiktoken encoding: {e}")
            self.available = False
            self.simple_counter = SimpleTokenCounter()
    
    def count(self, text: str) -> int:
        """Count tokens using tiktoken."""
        if not self.available:
            return self.simple_counter.count(text)
        return len(self.encoding.encode(text))
    
    def count_messages(self, messages: List[Dict[str, str]]) -> int:
        """
        Count tokens in message list.
        
        Based on OpenAI's token counting guide:
        https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
        """
        if not self.available:
            return self.simple_counter.count_messages(messages)
        
        num_tokens = 0
        for message in messages:
            # Every message follows <|start|>{role/name}\n{content}<|end|>\n
            num_tokens += 4
            for key, value in message.items():
                num_tokens += len(self.encoding.encode(value))
                if key == "name":
                    num_tokens += -1  # Role is omitted if name is present
        num_tokens += 2  # Every reply is primed with <|start|>assistant
        return num_tokens


def create_counter(
    strategy: str = "simple",
    model: Optional[str] = None
) -> TokenCounter:
    """
    Factory function to create appropriate token counter.
    
    Args:
        strategy: "simple" or "tiktoken"
        model: Model name for tiktoken (optional)
    
    Returns:
        TokenCounter instance
    """
    if strategy == "tiktoken":
        return TiktokenCounter(model or "gpt-4")
    else:
        return SimpleTokenCounter()
```

#### è®¾è®¡è¯´æ˜
- **ä¸¤ç§ç­–ç•¥**: Simpleï¼ˆå¿«é€Ÿä½†ç²—ç•¥ï¼‰å’ŒTiktokenï¼ˆæ…¢ä½†å‡†ç¡®ï¼‰
- **é™çº§å¤„ç†**: å¦‚æœtiktokenä¸å¯ç”¨ï¼Œè‡ªåŠ¨é™çº§åˆ°simple
- **å…¼å®¹æ€§**: æ”¯æŒæ‰€æœ‰ä¸»æµæ¨¡å‹çš„encoding

---

### 2. Compactioné…ç½®

#### æ–‡ä»¶: `agent/config.py`

```python
"""
Configuration for HIC Agent framework.
"""

from typing import Optional
from dataclasses import dataclass


@dataclass
class CompactionConfig:
    """Configuration for context compaction."""
    
    # æ˜¯å¦å¯ç”¨è‡ªåŠ¨compaction
    enabled: bool = True
    
    # Tokenè®¡æ•°ç­–ç•¥: "simple" or "tiktoken"
    token_counter: str = "simple"
    
    # è§¦å‘é˜ˆå€¼ï¼ˆå ç”¨contextçš„ç™¾åˆ†æ¯”ï¼‰
    # ä¾‹å¦‚ï¼š0.8 è¡¨ç¤ºå½“ä½¿ç”¨80%çš„contextæ—¶è§¦å‘
    threshold: float = 0.75
    
    # æ¨¡å‹çš„context limitï¼ˆå¦‚æœä¸º0ï¼Œä»LLMé…ç½®è·å–ï¼‰
    # æ ¼å¼: {"gpt-4": 128000, "claude-sonnet": 200000}
    context_limits: dict = None
    
    # ä¿æŠ¤æœ€è¿‘Næ¡æ¶ˆæ¯ä¸è¢«å‹ç¼©
    protect_recent_messages: int = 2
    
    # é¢„ç•™ç»™è¾“å‡ºçš„tokenæ•°
    reserved_output_tokens: int = 4096
    
    # Compactionå¤±è´¥åçš„é‡è¯•æ¬¡æ•°
    max_retries: int = 1
    
    # Compactionä½¿ç”¨çš„æ¨¡å‹ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨å½“å‰LLMçš„æ¨¡å‹ï¼‰
    compaction_model: Optional[str] = None
    
    def __post_init__(self):
        if self.context_limits is None:
            # é»˜è®¤çš„context limits
            self.context_limits = {
                "gpt-4": 128000,
                "gpt-4o": 128000,
                "gpt-3.5-turbo": 16385,
                "claude-sonnet-4.5": 200000,
                "claude-haiku-4.5": 200000,
                "deepseek-chat": 64000,
            }
    
    def get_context_limit(self, model_name: str) -> int:
        """
        Get context limit for a specific model.
        
        Args:
            model_name: Model name
        
        Returns:
            Context limit in tokens, or 0 if unknown
        """
        # Try exact match
        if model_name in self.context_limits:
            return self.context_limits[model_name]
        
        # Try partial match (e.g., "gpt-4-turbo" matches "gpt-4")
        for key, limit in self.context_limits.items():
            if model_name.startswith(key):
                return limit
        
        # Unknown model
        return 0
    
    def get_usable_tokens(self, model_name: str) -> int:
        """
        Get usable token count (context_limit - reserved_for_output).
        
        Args:
            model_name: Model name
        
        Returns:
            Usable tokens for input
        """
        limit = self.get_context_limit(model_name)
        if limit == 0:
            return 0
        return limit - self.reserved_output_tokens


# Global compaction config instance
_compaction_config = CompactionConfig()


def get_compaction_config() -> CompactionConfig:
    """Get the global compaction configuration."""
    return _compaction_config


def set_compaction_config(config: CompactionConfig):
    """Set the global compaction configuration."""
    global _compaction_config
    _compaction_config = config
```

#### é…ç½®è¯´æ˜
- **threshold**: é»˜è®¤0.75ï¼Œå³ä½¿ç”¨75%æ—¶è§¦å‘ï¼ˆç•™25%ä½™é‡ï¼‰
- **protect_recent_messages**: ä¿æŠ¤æœ€è¿‘2æ¡æ¶ˆæ¯ï¼Œé¿å…å‹ç¼©ä¸¢å¤±å³æ—¶ä¿¡æ¯
- **reserved_output_tokens**: ä¸ºLLMè¾“å‡ºé¢„ç•™4K tokens
- **context_limits**: å†…ç½®ä¸»æµæ¨¡å‹çš„é™åˆ¶ï¼Œå¯æ‰©å±•

---

### 3. Compactionæ ¸å¿ƒæ¨¡å—

#### æ–‡ä»¶: `agent/compaction.py`

```python
"""
Context compaction for HIC Agent framework.

Automatically summarizes conversation history when approaching token limits.
"""

import asyncio
from typing import List, Dict, Optional, Tuple
from agent.llm import LLM
from agent.token_counter import create_counter, TokenCounter
from agent.config import get_compaction_config, CompactionConfig


# Compaction system prompt
COMPACTION_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨è´Ÿè´£æ€»ç»“å¯¹è¯å†å²çš„AIåŠ©æ‰‹ã€‚

å½“è¢«è¦æ±‚æ€»ç»“æ—¶ï¼Œè¯·æä¾›ä¸€ä¸ªè¯¦ç»†ä½†ç®€æ´çš„æ€»ç»“ï¼Œé‡ç‚¹å…³æ³¨ï¼š
- å·²å®Œæˆçš„å·¥ä½œå’Œä»»åŠ¡
- å½“å‰æ­£åœ¨è¿›è¡Œçš„å·¥ä½œ
- æ¶‰åŠçš„æ–‡ä»¶å’Œä»£ç ä¿®æ”¹
- æ¥ä¸‹æ¥éœ€è¦åšä»€ä¹ˆ
- ç”¨æˆ·çš„å…³é”®è¦æ±‚ã€é™åˆ¶å’Œåå¥½
- é‡è¦çš„æŠ€æœ¯å†³ç­–åŠå…¶åŸå› 

ä½ çš„æ€»ç»“åº”è¯¥è¶³å¤Ÿå…¨é¢ä»¥æä¾›å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼Œä½†åˆè¶³å¤Ÿç®€æ´ä»¥ä¾¿å¿«é€Ÿç†è§£ã€‚
æ€»ç»“åº”è¯¥ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼ˆ"æˆ‘"ï¼‰çš„è§†è§’ï¼Œå°±åƒä½ åœ¨ç»§ç»­å¯¹è¯ä¸€æ ·ã€‚

é‡è¦ï¼š
- ä¸è¦é—æ¼å…³é”®çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚æ–‡ä»¶è·¯å¾„ã€å‡½æ•°åã€é”™è¯¯ä¿¡æ¯ç­‰ï¼‰
- ä¸è¦æ·»åŠ æ–°çš„å»ºè®®æˆ–è®¡åˆ’ï¼Œåªæ€»ç»“å·²æœ‰å†…å®¹
- ä¿æŒæ€»ç»“çš„è¿è´¯æ€§ï¼Œè®©ä¸‹ä¸€ä¸ªå¯¹è¯èƒ½å¤Ÿæ— ç¼ç»§ç»­
"""

COMPACTION_USER_PROMPT = """è¯·æ€»ç»“ä¸Šé¢çš„å¯¹è¯å†å²ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ‘˜è¦ã€‚

è¿™ä¸ªæ‘˜è¦å°†ç”¨äºç»§ç»­æˆ‘ä»¬çš„å¯¹è¯ï¼Œæ–°çš„å¯¹è¯å°†æ— æ³•è®¿é—®ä¸Šè¿°å†å²ï¼Œæ‰€ä»¥è¯·ç¡®ä¿æ‘˜è¦åŒ…å«æ‰€æœ‰å¿…è¦çš„ä¿¡æ¯ã€‚

è¯·ç”¨ç¬¬ä¸€äººç§°ï¼ˆ"æˆ‘"ï¼‰çš„è§†è§’æ¥å†™æ‘˜è¦ï¼Œå°±åƒä½ åœ¨ç»§ç»­å¯¹è¯ä¸€æ ·ã€‚ä¾‹å¦‚ï¼š
"æˆ‘å¸®åŠ©ç”¨æˆ·ä¿®å¤äº†Xæ–‡ä»¶ä¸­çš„Yé—®é¢˜ï¼Œé€šè¿‡Zæ–¹æ³•è§£å†³äº†..."

æ‘˜è¦åº”è¯¥åŒ…æ‹¬ï¼š
1. å¯¹è¯çš„èƒŒæ™¯å’Œç›®æ ‡
2. å·²å®Œæˆçš„ä¸»è¦ä»»åŠ¡å’Œç»“æœ
3. æ¶‰åŠçš„å…·ä½“æ–‡ä»¶å’Œä¿®æ”¹å†…å®¹
4. å½“å‰çš„çŠ¶æ€å’Œä¸‹ä¸€æ­¥è®¡åˆ’
5. éœ€è¦è®°ä½çš„é‡è¦çº¦æŸæˆ–ç”¨æˆ·åå¥½
"""


class CompactionDetector:
    """Detects when context compaction is needed."""
    
    def __init__(
        self,
        counter: TokenCounter,
        config: Optional[CompactionConfig] = None
    ):
        """
        Initialize compaction detector.
        
        Args:
            counter: Token counter instance
            config: Compaction configuration (uses global if None)
        """
        self.counter = counter
        self.config = config or get_compaction_config()
    
    def should_compact(
        self,
        history: List[Dict[str, str]],
        model_name: str
    ) -> Tuple[bool, int, int]:
        """
        Check if compaction is needed.
        
        Args:
            history: Chat history
            model_name: Model name for context limit lookup
        
        Returns:
            Tuple of (should_compact, current_tokens, limit_tokens)
        """
        if not self.config.enabled:
            return False, 0, 0
        
        # Get context limit
        usable = self.config.get_usable_tokens(model_name)
        if usable == 0:
            # Unknown model, can't determine
            return False, 0, 0
        
        # Count current tokens
        current = self.counter.count_messages(history)
        
        # Check threshold
        threshold = int(usable * self.config.threshold)
        should = current > threshold
        
        return should, current, usable


class CompactionAgent:
    """Executes context compaction by summarizing history."""
    
    def __init__(
        self,
        llm: LLM,
        counter: TokenCounter,
        config: Optional[CompactionConfig] = None
    ):
        """
        Initialize compaction agent.
        
        Args:
            llm: LLM instance for generating summaries
            config: Compaction configuration
        """
        self.llm = llm
        self.counter = counter
        self.config = config or get_compaction_config()
    
    async def compact(
        self,
        history: List[Dict[str, str]],
        protect_recent: Optional[int] = None
    ) -> List[Dict[str, str]]:
        """
        Compact chat history by generating a summary.
        
        Args:
            history: Original chat history
            protect_recent: Number of recent messages to protect (optional)
        
        Returns:
            Compacted history: [system, summary_message, ...protected_messages]
        
        Raises:
            RuntimeError: If compaction fails
        """
        if len(history) == 0:
            return history
        
        protect = protect_recent if protect_recent is not None else self.config.protect_recent_messages
        
        # Split history into: to_summarize and to_protect
        if protect > 0 and len(history) > protect:
            # Find the split point (keep system messages in to_summarize)
            protected_msgs = []
            to_summarize = []
            
            # Find system messages
            system_msgs = [msg for msg in history if msg.get("role") == "system"]
            
            # Get recent messages (excluding system)
            non_system = [msg for msg in history if msg.get("role") != "system"]
            if len(non_system) > protect:
                to_summarize_content = non_system[:-protect]
                protected_msgs = non_system[-protect:]
            else:
                # All non-system messages are protected
                to_summarize_content = []
                protected_msgs = non_system
            
            # Combine for summarization
            to_summarize = system_msgs + to_summarize_content
        else:
            to_summarize = history
            protected_msgs = []
        
        # Generate summary
        try:
            summary = await self._generate_summary(to_summarize)
        except Exception as e:
            raise RuntimeError(f"Failed to generate compaction summary: {e}")
        
        # Build new history
        # Keep system messages
        system_msgs = [msg for msg in history if msg.get("role") == "system"]
        
        # Create summary message
        summary_msg = {
            "role": "assistant",
            "content": f"[CONTEXT SUMMARY]\n\n{summary}"
        }
        
        # Combine: system + summary + protected
        compacted = system_msgs + [summary_msg] + protected_msgs
        
        return compacted
    
    async def _generate_summary(
        self,
        history: List[Dict[str, str]]
    ) -> str:
        """
        Generate summary using LLM.
        
        Args:
            history: History to summarize
        
        Returns:
            Summary text
        """
        # Save and reset LLM history
        original_history = self.llm.get_history()
        self.llm.reset_history()
        
        try:
            # Build compaction messages
            compaction_messages = history + [
                {
                    "role": "user",
                    "content": COMPACTION_USER_PROMPT
                }
            ]
            
            # Set history directly
            self.llm.set_history(compaction_messages)
            
            # Generate summary
            loop = asyncio.get_event_loop()
            summary = await loop.run_in_executor(
                None,
                self.llm.chat,
                COMPACTION_USER_PROMPT,
                COMPACTION_SYSTEM_PROMPT
            )
            
            return summary
        
        finally:
            # Restore original history
            self.llm.set_history(original_history)


async def compact_if_needed(
    llm: LLM,
    model_name: str,
    config: Optional[CompactionConfig] = None
) -> bool:
    """
    Check and perform compaction if needed.
    
    This is a convenience function that combines detection and compaction.
    
    Args:
        llm: LLM instance
        model_name: Model name for limit lookup
        config: Compaction configuration
    
    Returns:
        True if compaction was performed, False otherwise
    """
    config = config or get_compaction_config()
    
    # Create counter and detector
    counter = create_counter(
        strategy=config.token_counter,
        model=model_name
    )
    detector = CompactionDetector(counter, config)
    
    # Check if compaction needed
    history = llm.get_history()
    should, current, limit = detector.should_compact(history, model_name)
    
    if not should:
        return False
    
    # Log compaction
    try:
        from agent.async_logger import get_logger, LogLevel
        logger = get_logger()
        await logger.log(
            LogLevel.INFO,
            "compaction",
            f"ğŸ”„ Compaction triggered: {current}/{limit} tokens ({current*100//limit}%)",
            "COMPACTION"
        )
    except Exception:
        pass
    
    # Perform compaction
    agent = CompactionAgent(llm, counter, config)
    
    retries = config.max_retries
    last_error = None
    
    for attempt in range(retries + 1):
        try:
            compacted = await agent.compact(history)
            
            # Update LLM history
            llm.set_history(compacted)
            
            # Log success
            new_tokens = counter.count_messages(compacted)
            try:
                from agent.async_logger import get_logger, LogLevel
                logger = get_logger()
                await logger.log(
                    LogLevel.INFO,
                    "compaction",
                    f"âœ… Compaction complete: {current} â†’ {new_tokens} tokens (saved {current - new_tokens})",
                    "COMPACTION"
                )
            except Exception:
                pass
            
            return True
        
        except Exception as e:
            last_error = e
            if attempt < retries:
                # Log retry
                try:
                    from agent.async_logger import get_logger, LogLevel
                    logger = get_logger()
                    await logger.log(
                        LogLevel.WARNING,
                        "compaction",
                        f"âš ï¸  Compaction attempt {attempt + 1} failed: {e}. Retrying...",
                        "COMPACTION"
                    )
                except Exception:
                    pass
                
                await asyncio.sleep(1)  # Brief delay before retry
            else:
                # Log failure
                try:
                    from agent.async_logger import get_logger, LogLevel
                    logger = get_logger()
                    await logger.log(
                        LogLevel.ERROR,
                        "compaction",
                        f"âŒ Compaction failed after {retries + 1} attempts: {last_error}",
                        "COMPACTION"
                    )
                except Exception:
                    pass
                
                # Don't raise, let agent continue with full history
                return False
    
    return False
```

#### è®¾è®¡è¯´æ˜
- **CompactionDetector**: è´Ÿè´£æ£€æµ‹ï¼Œåˆ†ç¦»å…³æ³¨ç‚¹
- **CompactionAgent**: è´Ÿè´£æ‰§è¡Œï¼Œç”Ÿæˆæ€»ç»“
- **compact_if_needed**: ä¾¿æ·å‡½æ•°ï¼Œé›†æˆæ£€æµ‹+æ‰§è¡Œ+æ—¥å¿—
- **ä¿æŠ¤ç­–ç•¥**: ä¿ç•™æœ€è¿‘Næ¡æ¶ˆæ¯ï¼Œé¿å…ä¸¢å¤±å³æ—¶ä¿¡æ¯
- **å¤±è´¥å¤„ç†**: é‡è¯•æœºåˆ¶ï¼Œå¤±è´¥åä¸æŠ›å¼‚å¸¸ï¼ˆä¼˜é›…é™çº§ï¼‰

---

### 4. Agenté›†æˆ

#### æ–‡ä»¶: `agent/agent.py`

**ä¿®æ”¹ç‚¹1**: åœ¨`_internal_run`çš„ä¸»å¾ªç¯ä¸­æ·»åŠ compactionæ£€æµ‹

```python
# åœ¨ agent/agent.py çš„ _internal_run æ–¹æ³•ä¸­
# çº¦åœ¨ line 355 å·¦å³ï¼Œwhile iteration < self.max_iterations: ä¹‹å

async def _internal_run(self, task: str, agent_id: str) -> AgentResponse:
    # ... ç°æœ‰ä»£ç  ...
    
    while iteration < self.max_iterations:
        iteration += 1
        
        # [NEW] Check and perform compaction if needed
        try:
            from agent.compaction import compact_if_needed
            model_name = getattr(self.llm, 'model', 'unknown')
            await compact_if_needed(self.llm, model_name)
        except Exception as e:
            # Log but don't fail - compaction is best-effort
            try:
                from agent.async_logger import get_logger, LogLevel
                logger = get_logger()
                await logger.log(
                    LogLevel.WARNING,
                    agent_id,
                    f"âš ï¸  Compaction check failed: {e}",
                    "AGENT"
                )
            except Exception:
                pass
        
        # Try to parse LLM output (with retries)
        action = await self._parse_with_retry(llm_output, iteration, 3, agent_id)
        
        # ... ç°æœ‰ä»£ç ç»§ç»­ ...
```

**ä¿®æ”¹ç‚¹2**: åœ¨`_internal_resume`çš„å¾ªç¯ä¸­æ·»åŠ compactionæ£€æµ‹

```python
# åœ¨ agent/agent.py çš„ _internal_resume æ–¹æ³•ä¸­
# çº¦åœ¨ line 686 å·¦å³ï¼Œwhile iteration < self.max_iterations: ä¹‹å

async def _internal_resume(
    self, state: AgentState, message: AgentMessage
) -> AgentResponse:
    # ... ç°æœ‰ä»£ç  ...
    
    while iteration < self.max_iterations:
        iteration += 1
        
        # [NEW] Check and perform compaction if needed
        try:
            from agent.compaction import compact_if_needed
            model_name = getattr(self.llm, 'model', 'unknown')
            await compact_if_needed(self.llm, model_name)
        except Exception as e:
            # Log but don't fail
            try:
                from agent.async_logger import get_logger, LogLevel
                logger = get_logger()
                await logger.log(
                    LogLevel.WARNING,
                    agent_id,
                    f"âš ï¸  Compaction check failed: {e}",
                    "AGENT"
                )
            except Exception:
                pass
        
        # Parse LLM output
        action = await self._parse_with_retry(llm_output, iteration, 3, agent_id)
        
        # ... ç°æœ‰ä»£ç ç»§ç»­ ...
```

**ä¿®æ”¹ç‚¹3**: åœ¨LLMè°ƒç”¨å¤±è´¥å¤„ä¹Ÿæ£€æµ‹æ˜¯å¦å› ä¸ºcontextè¿‡é•¿

```python
# åœ¨ agent/agent.py çš„ _internal_run æ–¹æ³•ä¸­
# çº¦åœ¨ line 322 å·¦å³ï¼Œexcept Exception as e: é”™è¯¯å¤„ç†ä¸­

except Exception as e:
    # ... ç°æœ‰é”™è¯¯å¤„ç† ...
    
    # [NEW] Check if error is due to context length
    error_msg_lower = str(e).lower()
    if "context" in error_msg_lower and "length" in error_msg_lower:
        # Try emergency compaction
        try:
            from agent.compaction import compact_if_needed
            model_name = getattr(self.llm, 'model', 'unknown')
            compacted = await compact_if_needed(self.llm, model_name)
            if compacted:
                # Retry with compacted history
                try:
                    llm_output = await loop.run_in_executor(
                        None, self.llm.chat, task, self.system_prompt
                    )
                    # Continue processing...
                    continue
                except Exception as retry_e:
                    # Give up
                    error_msg = f"LLM call failed even after compaction: {retry_e}"
        except Exception as compact_e:
            # Log compaction failure
            pass
    
    # ... ç»§ç»­ç°æœ‰çš„é”™è¯¯å¤„ç† ...
```

#### é›†æˆè¯´æ˜
- **ä¸‰ä¸ªæ£€æµ‹ç‚¹**: 
  1. æ¯æ¬¡iterationå¼€å§‹æ—¶ï¼ˆé¢„é˜²æ€§ï¼‰
  2. resumeæ—¶ï¼ˆæ¢å¤åæ£€æµ‹ï¼‰
  3. LLMè°ƒç”¨å¤±è´¥æ—¶ï¼ˆåº”æ€¥ï¼‰
- **éä¾µå…¥å¼**: ä½¿ç”¨try-exceptåŒ…è£¹ï¼Œå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
- **ä¼˜é›…é™çº§**: Compactionå¤±è´¥æ—¶ç»§ç»­ä½¿ç”¨å®Œæ•´history

---

### 5. LLMæ‰©å±•

#### æ–‡ä»¶: `agent/llm.py`

**æ·»åŠ æ–¹æ³•**: ä¸º`LLM`åŸºç±»æ·»åŠ tokenè®¡æ•°æ–¹æ³•

```python
# åœ¨ agent/llm.py çš„ LLM ç±»ä¸­æ·»åŠ 

class LLM(ABC):
    # ... ç°æœ‰ä»£ç  ...
    
    def count_tokens(self, strategy: str = "simple") -> int:
        """
        Count tokens in current history.
        
        Args:
            strategy: "simple" or "tiktoken"
        
        Returns:
            Estimated token count
        """
        from agent.token_counter import create_counter
        
        model_name = getattr(self, 'model', 'gpt-4')
        counter = create_counter(strategy=strategy, model=model_name)
        return counter.count_messages(self.history)
    
    def get_context_usage(self, context_limit: int, strategy: str = "simple") -> float:
        """
        Get context usage as a percentage.
        
        Args:
            context_limit: Context window size in tokens
            strategy: "simple" or "tiktoken"
        
        Returns:
            Usage percentage (0.0 to 1.0+)
        """
        if context_limit == 0:
            return 0.0
        current = self.count_tokens(strategy)
        return current / context_limit
```

#### è¯´æ˜
- **å¯é€‰åŠŸèƒ½**: è¿™äº›æ–¹æ³•æ˜¯å¯é€‰çš„ï¼Œä¸»è¦ç”¨äºè°ƒè¯•å’Œç›‘æ§
- **ç­–ç•¥çµæ´»**: æ”¯æŒç®€å•å’Œç²¾ç¡®ä¸¤ç§è®¡æ•°æ–¹å¼
- **å‘åå…¼å®¹**: ä¸å½±å“ç°æœ‰LLMå®ç°

---

## æµ‹è¯•æ–¹æ¡ˆ

### å•å…ƒæµ‹è¯•

#### 1. TokenCounteræµ‹è¯•
**æ–‡ä»¶**: `tests/test_token_counter.py`

```python
import pytest
from agent.token_counter import SimpleTokenCounter, TiktokenCounter, create_counter


def test_simple_counter_basic():
    """Test simple counter basic functionality."""
    counter = SimpleTokenCounter()
    
    # Empty string
    assert counter.count("") == 0
    
    # Known lengths
    assert counter.count("a" * 4) == 1
    assert counter.count("a" * 8) == 2
    assert counter.count("a" * 100) == 25


def test_simple_counter_messages():
    """Test simple counter with message list."""
    counter = SimpleTokenCounter()
    
    messages = [
        {"role": "user", "content": "Hello"},
        {"role": "assistant", "content": "Hi there!"},
    ]
    
    # Should count role + content + overhead
    count = counter.count_messages(messages)
    assert count > 0


def test_tiktoken_counter():
    """Test tiktoken counter if available."""
    try:
        counter = TiktokenCounter(model="gpt-4")
        if counter.available:
            # Test basic counting
            text = "Hello, world!"
            count = counter.count(text)
            assert count > 0
            assert count < len(text)  # Should be fewer tokens than characters
    except ImportError:
        pytest.skip("tiktoken not installed")


def test_counter_factory():
    """Test counter factory function."""
    simple = create_counter("simple")
    assert isinstance(simple, SimpleTokenCounter)
    
    tiktoken = create_counter("tiktoken", model="gpt-4")
    assert tiktoken is not None
```

#### 2. CompactionDetectoræµ‹è¯•
**æ–‡ä»¶**: `tests/test_compaction_detector.py`

```python
import pytest
from agent.compaction import CompactionDetector
from agent.token_counter import SimpleTokenCounter
from agent.config import CompactionConfig


def test_detector_below_threshold():
    """Test detector when below threshold."""
    config = CompactionConfig(
        enabled=True,
        threshold=0.75,
        context_limits={"test-model": 1000},
        reserved_output_tokens=100
    )
    
    counter = SimpleTokenCounter()
    detector = CompactionDetector(counter, config)
    
    # Small history (far below threshold)
    history = [
        {"role": "user", "content": "Hi"},
        {"role": "assistant", "content": "Hello"},
    ]
    
    should, current, limit = detector.should_compact(history, "test-model")
    
    assert not should
    assert current < limit


def test_detector_above_threshold():
    """Test detector when above threshold."""
    config = CompactionConfig(
        enabled=True,
        threshold=0.75,
        context_limits={"test-model": 1000},
        reserved_output_tokens=100
    )
    
    counter = SimpleTokenCounter()
    detector = CompactionDetector(counter, config)
    
    # Large history (above threshold)
    # usable = 1000 - 100 = 900
    # threshold = 900 * 0.75 = 675
    # Need > 675 tokens, so create content > 675 * 4 = 2700 chars
    
    history = [
        {"role": "user", "content": "a" * 3000},
    ]
    
    should, current, limit = detector.should_compact(history, "test-model")
    
    assert should
    assert current > 675


def test_detector_disabled():
    """Test detector when compaction is disabled."""
    config = CompactionConfig(enabled=False)
    
    counter = SimpleTokenCounter()
    detector = CompactionDetector(counter, config)
    
    # Large history
    history = [
        {"role": "user", "content": "a" * 10000},
    ]
    
    should, current, limit = detector.should_compact(history, "test-model")
    
    assert not should
```

#### 3. CompactionAgentæµ‹è¯•
**æ–‡ä»¶**: `tests/test_compaction_agent.py`

```python
import pytest
from agent.compaction import CompactionAgent
from agent.token_counter import SimpleTokenCounter
from agent.config import CompactionConfig
from agent.llm import LLM
from typing import Optional


class MockLLM(LLM):
    """Mock LLM for testing."""
    
    def __init__(self):
        super().__init__()
        self.summary = "This is a summary of the conversation."
    
    def chat(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        # Add messages to history
        if not self.history and system_prompt:
            self.history.append({"role": "system", "content": system_prompt})
        self.history.append({"role": "user", "content": prompt})
        self.history.append({"role": "assistant", "content": self.summary})
        return self.summary


@pytest.mark.asyncio
async def test_compaction_agent_basic():
    """Test basic compaction."""
    llm = MockLLM()
    counter = SimpleTokenCounter()
    config = CompactionConfig()
    
    agent = CompactionAgent(llm, counter, config)
    
    # Create history
    history = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello"},
        {"role": "assistant", "content": "Hi there!"},
        {"role": "user", "content": "How are you?"},
        {"role": "assistant", "content": "I'm doing well, thanks!"},
    ]
    
    # Compact
    compacted = await agent.compact(history)
    
    # Should have: system + summary
    assert len(compacted) >= 2
    assert compacted[0]["role"] == "system"
    assert any("[CONTEXT SUMMARY]" in msg.get("content", "") for msg in compacted)


@pytest.mark.asyncio
async def test_compaction_agent_protect_recent():
    """Test compaction with protected messages."""
    llm = MockLLM()
    counter = SimpleTokenCounter()
    config = CompactionConfig(protect_recent_messages=2)
    
    agent = CompactionAgent(llm, counter, config)
    
    # Create history
    history = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Message 1"},
        {"role": "assistant", "content": "Response 1"},
        {"role": "user", "content": "Message 2"},
        {"role": "assistant", "content": "Response 2"},
    ]
    
    # Compact with protection
    compacted = await agent.compact(history, protect_recent=2)
    
    # Should have: system + summary + 2 protected messages
    assert len(compacted) >= 4
    
    # Check that recent messages are preserved
    assert compacted[-2]["content"] == "Message 2"
    assert compacted[-1]["content"] == "Response 2"
```

### é›†æˆæµ‹è¯•

#### æ–‡ä»¶: `tests/test_compaction_integration.py`

```python
import pytest
import asyncio
from agent.agent import Agent
from agent.llm import LLM
from agent.config import CompactionConfig, set_compaction_config
from typing import Optional


class TestLLM(LLM):
    """Test LLM that triggers compaction."""
    
    def __init__(self, trigger_at_call: int = 5):
        super().__init__()
        self.call_count = 0
        self.trigger_at_call = trigger_at_call
        self.model = "test-model"
    
    def chat(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        self.call_count += 1
        
        # Add to history
        if not self.history and system_prompt:
            self.history.append({"role": "system", "content": system_prompt})
        self.history.append({"role": "user", "content": prompt})
        
        # Generate response
        if "æ€»ç»“" in prompt or "summary" in prompt.lower():
            # This is a compaction request
            response = "Summary: This is a summary of previous conversation."
        else:
            response = f"Action: finish\nContent: Response {self.call_count}"
        
        self.history.append({"role": "assistant", "content": response})
        return response


@pytest.mark.asyncio
async def test_agent_with_compaction():
    """Test agent with compaction enabled."""
    # Configure compaction with low threshold
    config = CompactionConfig(
        enabled=True,
        threshold=0.5,  # Very low threshold for testing
        context_limits={"test-model": 100},  # Very small limit
        protect_recent_messages=1
    )
    set_compaction_config(config)
    
    # Create test LLM and agent
    llm = TestLLM()
    agent = Agent(
        llm=llm,
        name="TestAgent",
        max_iterations=10
    )
    
    # Run agent with a task that generates lots of history
    result = await agent._run_async("Test task that generates history")
    
    # Check that compaction occurred
    history = llm.get_history()
    
    # Should have summary in history
    has_summary = any("[CONTEXT SUMMARY]" in msg.get("content", "") for msg in history)
    
    # Due to low threshold, compaction should have been triggered
    # (This may not always trigger depending on timing, so we make it optional)
    print(f"History length: {len(history)}")
    print(f"Has summary: {has_summary}")
```

### æ€§èƒ½æµ‹è¯•

#### æ–‡ä»¶: `tests/test_compaction_performance.py`

```python
import pytest
import time
from agent.token_counter import SimpleTokenCounter, TiktokenCounter


def test_simple_counter_performance():
    """Test simple counter performance."""
    counter = SimpleTokenCounter()
    
    # Generate large text
    text = "a" * 100000
    
    start = time.time()
    count = counter.count(text)
    duration = time.time() - start
    
    # Should be very fast (< 10ms)
    assert duration < 0.01
    assert count > 0


def test_tiktoken_counter_performance():
    """Test tiktoken counter performance."""
    try:
        counter = TiktokenCounter(model="gpt-4")
        if not counter.available:
            pytest.skip("tiktoken not available")
        
        # Generate large text
        text = "a" * 100000
        
        start = time.time()
        count = counter.count(text)
        duration = time.time() - start
        
        # Should be reasonably fast (< 100ms)
        assert duration < 0.1
        assert count > 0
    except ImportError:
        pytest.skip("tiktoken not installed")
```

---

## é£é™©ä¸ç¼“è§£

### é£é™©1: Tokenè®¡æ•°ä¸å‡†ç¡®
**å½±å“**: å¯èƒ½è¿‡æ—©æˆ–è¿‡æ™šè§¦å‘compaction

**ç¼“è§£æªæ–½**:
- æä¾›ä¸¤ç§ç­–ç•¥ï¼ˆsimpleå’Œtiktokenï¼‰
- é»˜è®¤ä½¿ç”¨ä¿å®ˆçš„thresholdï¼ˆ0.75ï¼‰
- å…è®¸ç”¨æˆ·é…ç½®threshold

### é£é™©2: Compactionå¤±è´¥å¯¼è‡´agentå´©æºƒ
**å½±å“**: ç”¨æˆ·ä½“éªŒä¸­æ–­

**ç¼“è§£æªæ–½**:
- ä½¿ç”¨try-exceptåŒ…è£¹æ‰€æœ‰compactionè°ƒç”¨
- å¤±è´¥æ—¶ç»§ç»­ä½¿ç”¨å®Œæ•´historyï¼ˆä¼˜é›…é™çº§ï¼‰
- æä¾›é‡è¯•æœºåˆ¶ï¼ˆmax_retriesï¼‰
- è¯¦ç»†çš„é”™è¯¯æ—¥å¿—

### é£é™©3: Summaryä¸¢å¤±å…³é”®ä¿¡æ¯
**å½±å“**: åç»­å¯¹è¯ç¼ºå°‘å¿…è¦context

**ç¼“è§£æªæ–½**:
- ä½¿ç”¨è¯¦ç»†çš„compaction prompt
- ä¿æŠ¤æœ€è¿‘Næ¡æ¶ˆæ¯ä¸è¢«å‹ç¼©
- åœ¨summaryä¸­æ˜ç¡®è¦æ±‚ä¿ç•™æŠ€æœ¯ç»†èŠ‚
- æµ‹è¯•éªŒè¯summaryè´¨é‡

### é£é™©4: Compactionå¢åŠ å»¶è¿Ÿ
**å½±å“**: ç”¨æˆ·æ„Ÿè§‰agentå“åº”å˜æ…¢

**ç¼“è§£æªæ–½**:
- åªåœ¨å¿…è¦æ—¶è§¦å‘ï¼ˆthresholdæ§åˆ¶ï¼‰
- ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œ
- æä¾›æ¸…æ™°çš„æ—¥å¿—ï¼ˆç”¨æˆ·çŸ¥é“åœ¨åšä»€ä¹ˆï¼‰
- ä¼˜åŒ–compaction promptï¼ˆå‡å°‘LLMç”Ÿæˆæ—¶é—´ï¼‰

### é£é™©5: ä¸ç°æœ‰åŠŸèƒ½å†²çª
**å½±å“**: ç ´åç°æœ‰æµ‹è¯•æˆ–åŠŸèƒ½

**ç¼“è§£æªæ–½**:
- é»˜è®¤å¯ç”¨ä½†å¯é…ç½®å…³é—­
- éä¾µå…¥å¼é›†æˆï¼ˆç‹¬ç«‹æ¨¡å—ï¼‰
- å…¨é¢çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
- åˆ†é˜¶æ®µéƒ¨ç½²ï¼ˆå…ˆå†…éƒ¨æµ‹è¯•ï¼‰

---

## å®æ–½æ—¶é—´è¡¨

### Week 1: åŸºç¡€è®¾æ–½
- Day 1-2: å®ç°`TokenCounter`å’Œå•å…ƒæµ‹è¯•
- Day 3: å®ç°`CompactionConfig`
- Day 4: ä¸º`LLM`æ·»åŠ tokenè®¡æ•°æ–¹æ³•
- Day 5: Code reviewå’Œè°ƒæ•´

### Week 2: æ ¸å¿ƒåŠŸèƒ½
- Day 1-2: å®ç°`CompactionDetector`å’Œ`CompactionAgent`
- Day 3: ç¼–å†™compactionå•å…ƒæµ‹è¯•
- Day 4: é›†æˆåˆ°`Agent`
- Day 5: é›†æˆæµ‹è¯•

### Week 3: ä¼˜åŒ–å’Œæµ‹è¯•
- Day 1: ä¼˜åŒ–compaction prompt
- Day 2: æ·»åŠ è¯¦ç»†æ—¥å¿—
- Day 3: æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–
- Day 4: ç«¯åˆ°ç«¯æµ‹è¯•
- Day 5: æ–‡æ¡£å’Œcode review

### Week 4: éƒ¨ç½²å’Œç›‘æ§
- Day 1-2: å†…éƒ¨æµ‹è¯•å’Œbugä¿®å¤
- Day 3: éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
- Day 4: ç”¨æˆ·æµ‹è¯•
- Day 5: æ­£å¼å‘å¸ƒ

---

## æˆåŠŸæ ‡å‡†

### åŠŸèƒ½æ€§
- âœ… Tokenè®¡æ•°å‡†ç¡®ç‡ > 90%ï¼ˆå¯¹æ¯”å®é™…APIè¿”å›ï¼‰
- âœ… Compactionæ£€æµ‹æ­£ç¡®è§¦å‘ï¼ˆthresholdæµ‹è¯•ï¼‰
- âœ… SummaryåŒ…å«å…³é”®ä¿¡æ¯ï¼ˆäººå·¥è¯„ä¼°ï¼‰
- âœ… æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡
- âœ… æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡

### æ€§èƒ½
- âœ… Tokenè®¡æ•°è€—æ—¶ < 100msï¼ˆtiktokenï¼‰
- âœ… Compactionæ€»è€—æ—¶ < 10sï¼ˆåŒ…æ‹¬LLMè°ƒç”¨ï¼‰
- âœ… Agentå“åº”å»¶è¿Ÿå¢åŠ  < 5%

### å¯é æ€§
- âœ… Compactionå¤±è´¥ä¸å¯¼è‡´agentå´©æºƒ
- âœ… é”™è¯¯æ—¥å¿—æ¸…æ™°å¯è¯»
- âœ… é…ç½®å˜æ›´æ— éœ€é‡å¯

### ç”¨æˆ·ä½“éªŒ
- âœ… Compactionè¿‡ç¨‹æœ‰æ¸…æ™°æ—¥å¿—
- âœ… ç”¨æˆ·å¯ä»¥ç†è§£å‘ç”Ÿäº†ä»€ä¹ˆ
- âœ… å¯ä»¥é€šè¿‡é…ç½®ç¦ç”¨

---

## æœªæ¥æ‰©å±•

### Phase 4: Tool Output Pruningï¼ˆå¾…å®šï¼‰
- å®ç°tool outputçš„é€‰æ‹©æ€§æ¸…é™¤
- ä¿æŠ¤å…³é”®å·¥å…·çš„è¾“å‡º
- æ›´ç»†ç²’åº¦çš„tokenç®¡ç†

### Phase 5: æ™ºèƒ½Compactionï¼ˆå¾…å®šï¼‰
- æ ¹æ®å¯¹è¯ç±»å‹é€‰æ‹©ä¸åŒçš„compactionç­–ç•¥
- å­¦ä¹ å“ªäº›ä¿¡æ¯æ›´é‡è¦
- å¤šçº§å‹ç¼©ï¼ˆprogressive summarizationï¼‰

### Phase 6: å¯è§†åŒ–ï¼ˆå¾…å®šï¼‰
- åœ¨æ—¥å¿—ä¸­æ˜¾ç¤ºtokenä½¿ç”¨å›¾è¡¨
- Compactionå‰åçš„å¯¹æ¯”
- å®æ—¶tokenç›‘æ§

---

## é™„å½•

### A. ä¾èµ–åº“

```txt
# å¿…éœ€
# (æ— æ–°å¢ï¼Œä½¿ç”¨ç°æœ‰ä¾èµ–)

# å¯é€‰ï¼ˆç”¨äºç²¾ç¡®tokenè®¡æ•°ï¼‰
tiktoken>=0.5.0
```

### B. é…ç½®ç¤ºä¾‹

```python
# åœ¨ç”¨æˆ·ä»£ç ä¸­é…ç½®compaction
from agent.config import CompactionConfig, set_compaction_config

config = CompactionConfig(
    enabled=True,
    token_counter="tiktoken",  # ä½¿ç”¨ç²¾ç¡®è®¡æ•°
    threshold=0.8,  # 80%æ—¶è§¦å‘
    protect_recent_messages=3,  # ä¿æŠ¤æœ€è¿‘3æ¡æ¶ˆæ¯
    context_limits={
        "gpt-4": 128000,
        "custom-model": 50000,
    }
)

set_compaction_config(config)
```

### C. è°ƒè¯•æŠ€å·§

```python
# æŸ¥çœ‹å½“å‰tokenä½¿ç”¨
from agent.llm import CopilotLLM

llm = CopilotLLM()
# ... use llm ...

# Check token count
tokens = llm.count_tokens(strategy="tiktoken")
print(f"Current tokens: {tokens}")

# Check usage percentage
usage = llm.get_context_usage(context_limit=200000)
print(f"Context usage: {usage * 100:.1f}%")
```

### D. æ•…éšœæ’æŸ¥

**é—®é¢˜1**: Compactionæ€»æ˜¯è§¦å‘
- æ£€æŸ¥thresholdæ˜¯å¦å¤ªä½
- æ£€æŸ¥context_limitsé…ç½®æ˜¯å¦æ­£ç¡®
- ä½¿ç”¨tiktokenéªŒè¯å®é™…tokenæ•°

**é—®é¢˜2**: Compactionä»ä¸è§¦å‘
- æ£€æŸ¥enabledæ˜¯å¦ä¸ºTrue
- æ£€æŸ¥æ¨¡å‹åæ˜¯å¦åœ¨context_limitsä¸­
- å¢åŠ æ—¥å¿—æŸ¥çœ‹æ£€æµ‹é€»è¾‘

**é—®é¢˜3**: Summaryè´¨é‡å·®
- è°ƒæ•´COMPACTION_USER_PROMPT
- å¢åŠ protect_recent_messages
- ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹è¿›è¡Œcompaction

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2026-01-27  
**å®¡æ ¸çŠ¶æ€**: å¾…å®¡æ ¸
