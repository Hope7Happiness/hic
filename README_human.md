# LLM Agent Framework

> A type-safe, hierarchical LLM agent framework with **async parallel execution**, tool calling, and comprehensive logging.

> **ðŸ¤– For AI Assistants**: Need to set up GitHub Copilot? See [AI_COPILOT_SETUP.md](AI_COPILOT_SETUP.md) for detailed step-by-step instructions.

## TL;DR

Build AI agents that can use tools, delegate to sub-agents, and **run sub-agents in parallel** for maximum efficiency.

```python
from agent import DeepSeekLLM, Tool, Agent, get_deepseek_api_key

def calculator(expr: str) -> float:
    """Calculate a math expression."""
    return eval(expr)

# Get API key from .env file
api_key = get_deepseek_api_key()

# Create agent with tools
llm = DeepSeekLLM(api_key=api_key, model="deepseek-chat")
agent = Agent(llm=llm, tools=[Tool(calculator)])

# Run - logs automatically appear in console and files
response = agent.run("What is 25 * 4?")
```

**Key Features:**
- âš¡ **Async Parallel Execution** - Sub-agents run concurrently, not sequentially
- ðŸŽ¨ **Color-Coded Logging** - Hierarchical, indented output with timestamps
- ðŸ”§ **Type-Safe Tools** - Python functions with type hints become tools
- ðŸ¤– **Multi-LLM Support** - DeepSeek, OpenAI, or custom implementations
- ðŸ“Š **Comprehensive Logging** - Console + per-agent log files

## Setup

```bash
# Install uv if you haven't
curl -LsSf https://astral.sh/uv/install.sh | sh

# Setup environment
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install openai pydantic pyyaml python-dotenv pytest requests Levenshtein

# Configure API keys using .env (recommended)
cp .env.example .env
# Edit .env and add your API keys
```

### API Key Configuration

**Recommended: Use .env file**

```bash
# Create .env file from template
cp .env.example .env

# Edit .env and add your keys
nano .env  # or use any text editor
```

Your `.env` file should look like:
```bash
# DeepSeek API Key (recommended - cheaper and faster)
DEEPSEEK_API_KEY=sk-your_deepseek_key_here

# OpenAI API Key (optional)
OPENAI_API_KEY=sk-your_openai_key_here
```

**In Your Code:**

```python
from agent import get_deepseek_api_key, get_openai_api_key

# Automatically loads from .env or environment variables
deepseek_key = get_deepseek_api_key()
openai_key = get_openai_api_key()

if not deepseek_key:
    print("âŒ Please set DEEPSEEK_API_KEY in .env file")
```

### GitHub Copilot Setup

**ðŸ¤– Era of AI Assistants!**: Use an AI assistant to set up Copilot automatically! Just share [AI_COPILOT_SETUP.md](AI_COPILOT_SETUP.md) with your AI assistant and it will guide you through the entire process.

**Manual Setup:**

1. Create GitHub OAuth App at https://github.com/settings/developers
2. Enable device flow in the OAuth App settings
3. Run authentication:
   ```bash
   cd auth/copilot
   python cli.py auth login
   ```
4. Test:
   ```bash
   python cli.py models
   python examples/copilot_example.py
   ```

For detailed instructions, see [auth/copilot/README.md](auth/copilot/README.md).

## Run Examples

### ðŸŒŸ Async Parallel Agents (NEW!)

**The killer feature**: Multiple sub-agents running in parallel with real-time logging.

```bash
python examples/async_parallel_agents.py

# or: for a more realistic example, run 
# python examples/async_parallel_agents_real.py
```

**What it demonstrates:**
- âš¡ **Parallel Execution**: FastAgent (3s) and SlowAgent (8s) run simultaneously
- ðŸ“Š **Real-time Logging**: See agents start, execute, and complete with timestamps
- ðŸŽ¯ **Efficiency**: Total time ~15s (parallel) instead of ~25s (sequential)
- ðŸ”„ **State Management**: Parent agent suspends, waits, and resumes correctly

**Example Output:**
```
  0.052s [INFO] [ParentAgent] [AGENT] ðŸš€ Started with task: ...
  2.407s [INFO] [FastAgent] [AGENT] ðŸš€ Started with task: ç¡çœ 3ç§’
  2.410s [INFO] [SlowAgent] [AGENT] ðŸš€ Started with task: ç¡çœ 8ç§’
 11.618s [INFO] [FastAgent] [AGENT] âœ… Finished: å·²å®Œæˆ3ç§’ç¡çœ ä»»åŠ¡
 18.357s [INFO] [SlowAgent] [AGENT] âœ… Finished: å·²å®Œæˆ8ç§’ç¡çœ ä»»åŠ¡
 28.412s [INFO] [ParentAgent] [AGENT] âœ… Finished: æ‰€æœ‰å­Agentä»»åŠ¡æ‰§è¡Œå®Œæ¯•

âœ… Both agents started at nearly the same time - parallel execution confirmed!
   FastAgent: 3s sleep + ~7s LLM calls = ~10s total
   SlowAgent: 8s sleep + ~7s LLM calls = ~15s total
   Parallel execution means total = max(10s, 15s) = ~15s
   If sequential: 10s + 15s = 25s
```

**How it works:**
1. Parent agent uses `launch_subagents` to start multiple agents at once
2. Sub-agents execute in parallel (asyncio tasks)
3. Parent uses `wait_for_subagents` to suspend and wait
4. When a sub-agent completes, parent is notified and resumes
5. Parent checks remaining agents and continues or finishes

See detailed documentation: [`examples/README_async.md`](examples/README_async.md)

## Run Tests

```bash
# Run all tests (including async tests)
pytest tests/ -v

# Run async-specific tests
pytest tests/test_async_basic.py -v

# Test real-time reporting behavior
pytest tests/test_realtime_reporting.py -v

# Test with specific LLM
pytest tests/test_realtime_reporting.py -k deepseek -v
pytest tests/test_realtime_reporting.py -k copilot -v

# Test Copilot authentication (requires Copilot setup)
pytest tests/test_copilot_auth.py -v
# Or run directly for detailed output:
python tests/test_copilot_auth.py

# Run fast tests only (skip LLM API calls)
pytest tests/ -v -m "not integration"
```

### Real-Time Reporting Test

The `test_realtime_reporting.py` is a critical test that ensures agents provide real-time feedback to users, not batch results at the end.

**Why it matters:**
Imagine you ask an agent to check weather (3 seconds) and stock prices (10 seconds). You don't want to wait 13 seconds for both results - you want to see the weather immediately when it's ready!

**What the test does:**
1. Parent agent receives: "æŸ¥è¯¢åŒ—äº¬å¤©æ°”å’Œè‹¹æžœè‚¡ç¥¨ä»·æ ¼" (Check Beijing weather and Apple stock)
2. Parent launches 2 sub-agents in parallel:
   - WeatherAgent (3s) - Fast task
   - StockAgent (10s) - Slow task
3. WeatherAgent finishes first â†’ Parent IMMEDIATELY reports weather data (temperature, conditions, location)
4. Parent continues waiting for StockAgent (doesn't finish early)
5. StockAgent finishes â†’ Parent reports stock data
6. Parent finishes with complete summary

**Key validation:**
- Checks for ACTUAL weather data (not just "WeatherAgent" name):
  - Weather conditions: æ™´å¤©, é˜´å¤©, cloudy, etc.
  - Temperature: 15Â°C, æ°”æ¸©ï¼š20åº¦, etc.
  - Location: åŒ—äº¬, Beijing
- All 12 workflow steps must occur in correct order
- Parent must report results incrementally, not batch at end

**Run the test:**
```bash
# Test with both LLMs
pytest tests/test_realtime_reporting.py -v

# Test with specific LLM
pytest tests/test_realtime_reporting.py -k deepseek -v
pytest tests/test_realtime_reporting.py -k copilot -v
```

**Technical features demonstrated:**
- Async parallel execution (not sequential)
- Real-time incremental reporting
- Error handling for API failures (429 rate limits)
- Independent LLM instances per agent (prevents history contamination)
- Strict log validation

### Peer-to-Peer Communication Test (NEW)

`tests/test_communicate.py` ensures sibling agents can exchange data directly via `send_message`/`wait`, even when one agent is still running. AgentA knows theå“ˆå¸Œå‰åŠéƒ¨åˆ†, AgentBçŸ¥é“åŽåŠéƒ¨åˆ†â€”â€”ä»–ä»¬å¿…é¡»äº’ç›¸åŒæ­¥ã€ç¡®è®¤åŒæ–¹éƒ½æŽŒæ¡å®Œæ•´å“ˆå¸Œç åŽå† finishã€‚

```bash
# é»˜è®¤ä½¿ç”¨çœŸå®ž DeepSeek LLMï¼ˆéœ€è¦ DEEPSEEK_API_KEYï¼‰
pytest tests/test_communicate.py -s -k deepseek -v

# å¦‚éœ€ç¦»çº¿/æ—  API æ¨¡å¼ï¼Œä½¿ç”¨è„šæœ¬åŒ– LLM
USE_SCRIPTED_LLM=1 pytest tests/test_communicate.py -s -k deepseek -v
```

æ—¥å¿—ä¼šå‡ºçŽ° `[AgentA -> AgentB]å‘é€ä¿¡æ¯ï¼Œå¯¹æ–¹çŠ¶æ€æ˜¯waitï¼Œä¿¡æ¯å†…å®¹ï¼š...` ç­‰è¯­å¥ï¼Œä¾¿äºŽåœ¨ `logs/AgentA_*.log`ã€`logs/AgentB_*.log`ã€`logs/ParentAgent_*.log` ä¸­è¿½è¸ªæ•´ä¸ªé€šä¿¡è¿‡ç¨‹ã€‚

### å¹¶è¡ŒçŒœæ•°æŒ‘æˆ˜ (NEW)

`tests/test_parallel_guess.py` ä¼šå¹¶è¡Œå¯åŠ¨ 6 ä¸ªå­ Agentï¼ˆä¸‰ä¸ªæé—®è€… + ä¸‰ä¸ªå›žç­”è€…ï¼‰ä»¥åŠä¸€ä¸ªçˆ¶ Agentï¼š

- çˆ¶ Agent é€‰æ‹© 3 ä¸ª 1-10 çš„æ•´æ•°ï¼Œå¹¶æŠŠå®ƒä»¬åˆ†å‘ç»™å›žç­”è€…
- æ¯ä¸ªæé—®è€…åªèƒ½é—® â€œçœŸå®žæ•°å­—æ¯” X å¤§/å°/ç­‰äºŽï¼Ÿâ€
- å›žç­”è€…å¿…é¡»è¯šå®žå›žç­”ï¼ˆæ”¯æŒå¤§/å°/ç›¸ç­‰ç­‰ä¸‰ç§æƒ…å†µï¼‰
- æé—®è€…çŒœå¯¹åŽç«‹åˆ» finishï¼Œçˆ¶ Agent éœ€è¦æ ¹æ®å®Œæˆå…ˆåŽç»™å‡ºæŽ’å
- æµ‹è¯•ä¼šè§£æž `logs/Questioner*_*.log` çš„å®Œæˆæ—¶é—´ï¼Œç¡®ä¿çˆ¶ Agent æŠ¥å‘Šçš„æŽ’åä¸ŽçœŸå®žå®Œæˆé¡ºåºä¸€è‡´

```bash
# é»˜è®¤ä½¿ç”¨çœŸå®ž DeepSeek LLMï¼ˆéœ€è¦ DEEPSEEK_API_KEYï¼‰
pytest tests/test_parallel_guess.py -s -k deepseek -v

# å¦‚éœ€ç¦»çº¿/æ—  API æ¨¡å¼ï¼Œä½¿ç”¨è„šæœ¬åŒ– LLM
USE_SCRIPTED_LLM=1 pytest tests/test_parallel_guess.py -v
```

è¯¥æµ‹è¯•è¦†ç›–äº†å¤šå¯¹ç­‰ Agent åŒæ—¶é€šä¿¡ã€æŽ’é˜Ÿæ¶ˆæ¯åœ¨ wait/è¿è¡ŒçŠ¶æ€ä¹‹é—´åˆ‡æ¢ã€ä»¥åŠä»Žæ—¥å¿—æå–æ—¶é—´æˆ³åšæ–­è¨€çš„å®Œæ•´æµç¨‹ã€‚

## Key Features Explained

### 1. Async Parallel Execution

**Problem:** Traditional agent frameworks run sub-agents sequentially, wasting time when tasks are independent.

**Solution:** Our async framework allows sub-agents to run in parallel:

```python
# Parent agent can launch multiple sub-agents at once
Action: launch_subagents
Agents: ["DataFetcher", "CacheChecker", "Validator"]
Tasks: ["Fetch from API", "Check cache", "Validate input"]

# All three run in parallel!
# Total time = max(task1, task2, task3), not sum(task1, task2, task3)
```

### 2. Comprehensive Logging

**By default, logs automatically appear in console and files.** The AsyncLogger is automatically initialized when you run an agent.

**Console Output** (color-coded, hierarchical):
```
  0.052s [INFO] [ParentAgent] [AGENT] ðŸš€ Started
  2.407s [INFO]   [FastAgent] [AGENT] ðŸš€ Started  â† Indented for hierarchy
  2.410s [INFO]   [SlowAgent] [AGENT] ðŸš€ Started
```

**Log Files** (per-agent, timestamped):
```
logs/
â”œâ”€â”€ ParentAgent_20260125_224241_*.log
â”œâ”€â”€ FastAgent_20260125_224243_*.log
â””â”€â”€ SlowAgent_20260125_224243_*.log
```

**Disable Console Logging** (keep file logs only):
```python
from agent.async_logger import init_logger

# Initialize logger with console output disabled
await init_logger(console_output=False)

# Now run your agent - logs go to files only
agent = Agent(llm=llm, tools=[tools])
result = await agent._run_async("Your task")
```

### 3. Type-Safe Tools

Tools are just Python functions with type hints:

```python
def search_database(query: str, limit: int = 10) -> list[dict]:
    """Search the database with a query string."""
    # Implementation here
    return results

# Automatically becomes a tool
tool = Tool(search_database)

# Agent can call it with validated arguments
agent = Agent(llm=llm, tools=[tool])
```

### 4. Hierarchical Agents

Agents can delegate to specialized sub-agents:

```python
# Parent coordinates, sub-agents specialize
parent = Agent(
    llm=llm,
    subagents={
        "researcher": research_agent,
        "writer": writing_agent,
        "reviewer": review_agent,
    }
)
```

## Architecture

### Core Components

1. **Agent** (`agent/agent.py`)
   - Main execution loop with async support
   - Parses LLM output into actions
   - Executes tools and manages sub-agents
   - Handles suspension and resumption

2. **AgentOrchestrator** (`agent/orchestrator.py`)
   - Singleton coordinator for all agents
   - Manages async tasks and message queue
   - Handles agent registration and lifecycle
   - Tracks parent-child relationships

3. **AsyncLogger** (`agent/async_logger.py`)
   - Non-blocking file I/O
   - Color-coded console output
   - Per-agent log files
   - Hierarchical indentation

4. **Tools** (`agent/tool.py`)
   - Wraps Python functions
   - Validates arguments with Pydantic
   - Generates tool descriptions for LLM

5. **Schemas** (`agent/schemas.py`)
   - `Action`: Parsed LLM output (tool, launch_subagents, wait_for_subagents, finish)
   - `AgentState`: Serializable state for suspension/resumption
   - `AgentMessage`: Inter-agent communication
   - `LaunchedSubagent`: Tracks sub-agent execution

### Execution Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Parent Agent                          â”‚
â”‚  1. LLM returns: launch_subagents ["AgentA", "AgentB"]      â”‚
â”‚  2. Launch both agents (non-blocking)                        â”‚
â”‚  3. LLM returns: wait_for_subagents                          â”‚
â”‚  4. Save state and SUSPEND                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Agent A       â”‚                   â”‚    Agent B       â”‚
â”‚  (running...)    â”‚                   â”‚  (running...)    â”‚
â”‚                  â”‚                   â”‚                  â”‚
â”‚  âœ… Completes    â”‚                   â”‚  (still running) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                       â”‚
        â”‚ Send "completed" message              â”‚
        â–¼                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Parent Agent (RESUMED)                    â”‚
â”‚  Receives: AgentA completed                                  â”‚
â”‚  Status: AgentA âœ…, AgentB ðŸ”„                               â”‚
â”‚  Decision: Continue waiting                                  â”‚
â”‚  LLM returns: wait_for_subagents                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Wait for AgentB...
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Parent Agent (RESUMED AGAIN)              â”‚
â”‚  Receives: AgentB completed                                  â”‚
â”‚  Status: AgentA âœ…, AgentB âœ…                               â”‚
â”‚  Decision: All done!                                         â”‚
â”‚  LLM returns: finish                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
hic/
â”œâ”€â”€ agent/                      # Core framework
â”‚   â”œâ”€â”€ llm.py                 # LLM abstract class + OpenAI implementation
â”‚   â”œâ”€â”€ deepseek_llm.py        # DeepSeek LLM implementation
â”‚   â”œâ”€â”€ tool.py                # Tool system (Python functions â†’ tools)
â”‚   â”œâ”€â”€ agent.py               # Agent execution logic (async support)
â”‚   â”œâ”€â”€ orchestrator.py        # Async coordinator singleton
â”‚   â”œâ”€â”€ async_logger.py        # Async-safe logging system
â”‚   â”œâ”€â”€ parser.py              # LLM output parser (supports new actions)
â”‚   â”œâ”€â”€ schemas.py             # Pydantic data models
â”‚   â”œâ”€â”€ callbacks.py           # Callback system for observability
â”‚   â”œâ”€â”€ config.py              # API key configuration with .env support
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ test_async_basic.py    # Async parallel execution tests
â”‚   â”œâ”€â”€ test_realtime_reporting.py  # Real-time reporting behavior tests
â”‚   â”œâ”€â”€ test_tool.py           # Tool creation & validation
â”‚   â”œâ”€â”€ test_llm.py            # LLM implementations tests
â”‚   â”œâ”€â”€ test_llm_abstract.py   # Abstract LLM base class tests
â”‚   â”œâ”€â”€ test_skill.py          # YAML skill loading tests
â”‚   â”œâ”€â”€ test_copilot_auth.py   # Copilot authentication tests
â”‚   â”œâ”€â”€ test_utils.py          # Utility functions tests
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ examples/                   # Usage examples
â”‚   â”œâ”€â”€ async_parallel_agents.py    # ðŸŒŸ Async parallel execution demo
â”‚   â”œâ”€â”€ README_async.md             # Detailed async documentation
â”‚   â”œâ”€â”€ simple_agent.py             # Basic agent (best for beginners)
â”‚   â”œâ”€â”€ zoo_director.py             # Hierarchical agents
â”‚   â”œâ”€â”€ deepseek_agent.py           # Agent with DeepSeek LLM
â”‚   â””â”€â”€ custom_llm.py               # Custom LLM implementation
â”‚
â”œâ”€â”€ logs/                       # Generated log files (per-agent)
â”œâ”€â”€ .env.example               # API key configuration template
â””â”€â”€ pyproject.toml             # Project config
```

## Advanced Usage

### Custom Tools

```python
from agent import Tool

def fetch_data(url: str, timeout: int = 30) -> dict:
    """Fetch data from a URL with optional timeout."""
    import requests
    response = requests.get(url, timeout=timeout)
    return response.json()

tool = Tool(fetch_data)
agent = Agent(llm=llm, tools=[tool])
```

### Custom LLM

```python
from agent import LLM

class MyCustomLLM(LLM):
    def chat(self, prompt: str, system_prompt: str | None = None) -> str:
        # Your implementation
        return response
    
    def reset_history(self):
        self.history = []
    
    def get_history(self):
        return self.history
    
    def set_history(self, history):
        self.history = history

llm = MyCustomLLM()
agent = Agent(llm=llm)
```

### Async API

```python
import asyncio
from agent import Agent, init_logger, close_logger

async def main():
    # Initialize logger
    logger = await init_logger(log_dir="logs")
    
    # Create agent
    agent = Agent(llm=llm, tools=[tool1, tool2])
    
    # Run async (use this inside async context)
    result = await agent._run_async(task="Your task here")
    
    # Close logger
    await close_logger()

asyncio.run(main())
```

## Contributing

Contributions are welcome! Areas for improvement:
- Additional LLM providers (Anthropic, Cohere, etc.)
- More sophisticated planning algorithms
- Enhanced error recovery
- Performance optimizations
- Additional examples and tutorials

## License

MIT License - see LICENSE file for details.
