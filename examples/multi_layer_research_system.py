"""
å¤æ‚çš„ä¸‰å±‚ Agent ç³»ç»Ÿç¤ºä¾‹ - ç ”ç©¶åŠ©ç†ç³»ç»Ÿ

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†ï¼š
1. ä¸‰å±‚ Agent æ¶æ„ï¼š
   - L1 (é¡¶å±‚): ResearchDirector - ç ”ç©¶æ€»ç›‘
   - L2 (ä¸­å±‚): DataAnalyst, LiteratureResearcher - æ•°æ®åˆ†æå¸ˆã€æ–‡çŒ®ç ”ç©¶å‘˜
   - L3 (åº•å±‚): DataCollector, DataProcessor, PaperFinder, SummaryGenerator - æ•°æ®é‡‡é›†/å¤„ç†å‘˜ã€è®ºæ–‡æŸ¥æ‰¾/æ‘˜è¦å‘˜

2. å¤šè½®è°ƒç”¨ï¼š
   - æ¯ä¸ª agent å¯ä»¥å¤šæ¬¡è°ƒç”¨ tools å’Œ subagents
   - çˆ¶ agent å¯ä»¥ç­‰å¾…å­ agent å®Œæˆåç»§ç»­æ‰§è¡Œ

3. é€’å½’è°ƒç”¨ï¼š
   - DataAnalyst è°ƒç”¨ DataCollector å’Œ DataProcessor
   - LiteratureResearcher è°ƒç”¨ PaperFinder å’Œ SummaryGenerator
   - æ‰€æœ‰è°ƒç”¨éƒ½æ˜¯å¼‚æ­¥å¹¶è¡Œæ‰§è¡Œ

æ¶æ„å›¾ï¼š
                    ResearchDirector (L1)
                    /
        DataAnalyst (L2)        LiteratureResearcher (L2)
           /                          /
 DataCollector  DataProcessor   PaperFinder    SummaryGenerator
     (L3)           (L3)            (L3)              (L3)

ä½¿ç”¨åœºæ™¯ï¼š
ç ”ç©¶æ€»ç›‘æ¥æ”¶ä¸€ä¸ªç ”ç©¶ä»»åŠ¡ï¼Œå°†å…¶åˆ†è§£ä¸ºæ•°æ®åˆ†æå’Œæ–‡çŒ®ç ”ç©¶ä¸¤éƒ¨åˆ†ã€‚
æ•°æ®åˆ†æå¸ˆè´Ÿè´£æ•°æ®æ”¶é›†å’Œå¤„ç†ï¼Œæ–‡çŒ®ç ”ç©¶å‘˜è´Ÿè´£æŸ¥æ‰¾è®ºæ–‡å’Œç”Ÿæˆæ‘˜è¦ã€‚
"""

import asyncio
import time
import json
from typing import Dict, List, Any
from agent.agent import Agent
from agent.llm import DeepSeekLLM
from agent.tool import Tool
from agent.async_logger import init_logger, close_logger
from agent.config import load_env, get_deepseek_api_key


# ============================================================================
# Layer 3 Tools (åº•å±‚å·¥å…·) - å®é™…æ‰§è¡Œå…·ä½“ä»»åŠ¡
# ============================================================================


def fetch_data_from_api(api_name: str, params: str) -> str:
    """
    ä»æŒ‡å®š API è·å–æ•°æ®

    Args:
        api_name: API åç§° (åªèƒ½ä¸º "weather_api", "stock_api", "census_api")
        params: API å‚æ•° (JSON æ ¼å¼å­—ç¬¦ä¸²)

    Returns:
        API è¿”å›çš„æ•°æ® (JSON å­—ç¬¦ä¸²)
    """
    time.sleep(1)  # æ¨¡æ‹Ÿ API è°ƒç”¨å»¶è¿Ÿ

    # æ¨¡æ‹Ÿä¸åŒ API è¿”å›ä¸åŒæ•°æ®
    mock_data = {
        "weather_api": {"temperature": 25, "humidity": 60, "condition": "sunny"},
        "stock_api": {"symbol": "AAPL", "price": 178.50, "change": +2.3},
        "census_api": {"population": 1400000000, "growth_rate": 0.5},
    }

    result = mock_data.get(api_name, {"error": "API not found"})
    return json.dumps(result, ensure_ascii=False)


def scrape_website(url: str, selector: str) -> str:
    """
    ä»ç½‘ç«™æŠ“å–æ•°æ®

    Args:
        url: ç½‘ç«™ URL (åªèƒ½ä¸º "research.com", "data.gov", "arxiv.org")
        selector: CSS é€‰æ‹©å™¨

    Returns:
        æŠ“å–åˆ°çš„æ–‡æœ¬å†…å®¹
    """
    time.sleep(1.5)  # æ¨¡æ‹Ÿç½‘é¡µæŠ“å–å»¶è¿Ÿ

    # æ¨¡æ‹ŸæŠ“å–ç»“æœ
    mock_content = {
        "research.com": "Latest research shows AI is transforming healthcare...",
        "data.gov": "Government statistics indicate economic growth of 3.2%...",
        "arxiv.org": "Recent papers on machine learning and neural networks...",
    }

    for domain, content in mock_content.items():
        if domain in url:
            return content

    return "Sample scraped content from " + url


def clean_data(raw_data: str) -> str:
    """
    æ¸…æ´—å’Œé¢„å¤„ç†åŸå§‹æ•°æ®

    Args:
        raw_data: åŸå§‹æ•°æ® (JSON æˆ–æ–‡æœ¬)

    Returns:
        æ¸…æ´—åçš„æ•°æ®æè¿°
    """
    time.sleep(0.5)

    try:
        data = json.loads(raw_data)
        return f"Cleaned data: {len(data)} fields processed, normalized, and validated"
    except:
        return f"Cleaned text data: {len(raw_data)} characters, removed duplicates and invalid entries"


def transform_data(clean_data_desc: str, format: str) -> str:
    """
    è½¬æ¢æ•°æ®æ ¼å¼

    Args:
        clean_data_desc: æ¸…æ´—åçš„æ•°æ®æè¿°
        format: ç›®æ ‡æ ¼å¼ (ä¾‹å¦‚: "csv", "json", "table")

    Returns:
        è½¬æ¢åçš„æ•°æ®æè¿°
    """
    time.sleep(0.3)
    return f"Transformed data to {format} format: {clean_data_desc}"


def search_papers(query: str, max_results: int = 5) -> str:
    """
    æœç´¢å­¦æœ¯è®ºæ–‡

    Args:
        query: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°

    Returns:
        æ‰¾åˆ°çš„è®ºæ–‡åˆ—è¡¨ (JSON å­—ç¬¦ä¸²)
    """
    time.sleep(2)  # æ¨¡æ‹Ÿæœç´¢å»¶è¿Ÿ

    # æ¨¡æ‹Ÿæœç´¢ç»“æœ
    papers = [
        {
            "title": f"Deep Learning Applications in {query}",
            "authors": ["Zhang et al."],
            "year": 2024,
            "abstract": f"This paper explores novel applications of deep learning in {query}...",
        },
        {
            "title": f"A Survey on {query} Techniques",
            "authors": ["Smith et al."],
            "year": 2023,
            "abstract": f"We provide a comprehensive survey of current {query} methodologies...",
        },
        {
            "title": f"{query}: Challenges and Future Directions",
            "authors": ["Wang et al."],
            "year": 2024,
            "abstract": f"This work identifies key challenges in {query} and proposes future research directions...",
        },
    ][:max_results]

    return json.dumps(papers, ensure_ascii=False, indent=2)


def download_paper(paper_title: str) -> str:
    """
    ä¸‹è½½è®ºæ–‡å…¨æ–‡

    Args:
        paper_title: è®ºæ–‡æ ‡é¢˜

    Returns:
        è®ºæ–‡å†…å®¹æ‘˜è¦
    """
    time.sleep(1)
    return f"Downloaded paper: {paper_title}\nContent: Full text of {len(paper_title) * 100} words available."


def summarize_text(text: str, max_length: int = 200) -> str:
    """
    ç”Ÿæˆæ–‡æœ¬æ‘˜è¦

    Args:
        text: åŸå§‹æ–‡æœ¬
        max_length: æ‘˜è¦æœ€å¤§é•¿åº¦

    Returns:
        æ–‡æœ¬æ‘˜è¦
    """
    time.sleep(0.8)

    # ç®€å•æ¨¡æ‹Ÿæ‘˜è¦ç”Ÿæˆ
    summary = text[:max_length] + "..." if len(text) > max_length else text
    return f"Summary ({len(summary)} chars): {summary}"


def extract_key_findings(text: str) -> str:
    """
    æå–å…³é”®å‘ç°

    Args:
        text: æ–‡æœ¬å†…å®¹

    Returns:
        å…³é”®å‘ç°åˆ—è¡¨
    """
    time.sleep(0.5)

    # æ¨¡æ‹Ÿæå–å…³é”®å‘ç°
    findings = [
        "Key Finding 1: Significant improvement in accuracy (95%)",
        "Key Finding 2: Novel architecture reduces training time by 40%",
        "Key Finding 3: Generalizes well to multiple domains",
    ]

    return "\n".join(findings)


# ============================================================================
# Create All Tools
# ============================================================================


def create_all_tools() -> Dict[str, Tool]:
    """åˆ›å»ºæ‰€æœ‰å·¥å…·çš„å­—å…¸"""
    return {
        # Data collection tools
        "fetch_data_from_api": Tool(fetch_data_from_api),
        "scrape_website": Tool(scrape_website),
        # Data processing tools
        "clean_data": Tool(clean_data),
        "transform_data": Tool(transform_data),
        # Literature research tools
        "search_papers": Tool(search_papers),
        "download_paper": Tool(download_paper),
        # Summary generation tools
        "summarize_text": Tool(summarize_text),
        "extract_key_findings": Tool(extract_key_findings),
    }


# ============================================================================
# Layer 3 Agents (åº•å±‚ Agents) - æ‰§è¡Œå…·ä½“ä»»åŠ¡
# ============================================================================


def create_data_collector(api_key: str) -> Agent:
    """
    åˆ›å»ºæ•°æ®é‡‡é›†å‘˜ (L3)

    èŒè´£ï¼š
    - ä»å„ç§æ•°æ®æºæ”¶é›†åŸå§‹æ•°æ®
    - ä½¿ç”¨ API å’Œç½‘é¡µæŠ“å–å·¥å…·
    """
    llm = DeepSeekLLM(api_key=api_key, model="deepseek-chat")

    tools_dict = create_all_tools()
    tools = [tools_dict["fetch_data_from_api"], tools_dict["scrape_website"]]

    system_prompt = """ä½ æ˜¯æ•°æ®é‡‡é›†å‘˜ï¼ˆData Collectorï¼‰ã€‚

ä½ çš„èŒè´£ï¼š
1. ä»æŒ‡å®šçš„æ•°æ®æºæ”¶é›†åŸå§‹æ•°æ®
2. ä½¿ç”¨ fetch_data_from_api å·¥å…·ä» API è·å–æ•°æ®
3. ä½¿ç”¨ scrape_website å·¥å…·ä»ç½‘ç«™æŠ“å–æ•°æ®
4. è¿”å›æ”¶é›†åˆ°çš„æ‰€æœ‰åŸå§‹æ•°æ®

å·¥ä½œæµç¨‹ï¼š
1. åˆ†æä»»åŠ¡ï¼Œç¡®å®šéœ€è¦å“ªäº›æ•°æ®æº
2. è°ƒç”¨ç›¸åº”çš„å·¥å…·æ”¶é›†æ•°æ®
3. æ•´ç†å¹¶è¿”å›æ‰€æœ‰æ”¶é›†åˆ°çš„æ•°æ®

é‡è¦ï¼š
- æ”¶é›†å®Œæ‰€æœ‰æ•°æ®åï¼Œä½¿ç”¨ Action: finish è¿”å›ç»“æœ
- ç»“æœåº”è¯¥åŒ…å«æ‰€æœ‰æ•°æ®æºå’Œæ”¶é›†åˆ°çš„å†…å®¹
"""

    return Agent(
        llm=llm,
        tools=tools,
        name="DataCollector",
        system_prompt=system_prompt,
        max_iterations=5,
    )


def create_data_processor(api_key: str) -> Agent:
    """
    åˆ›å»ºæ•°æ®å¤„ç†å‘˜ (L3)

    èŒè´£ï¼š
    - æ¸…æ´—å’Œè½¬æ¢åŸå§‹æ•°æ®
    - å‡†å¤‡æ•°æ®ç”¨äºåˆ†æ
    """
    llm = DeepSeekLLM(api_key=api_key, model="deepseek-chat")

    tools_dict = create_all_tools()
    tools = [tools_dict["clean_data"], tools_dict["transform_data"]]

    system_prompt = """ä½ æ˜¯æ•°æ®å¤„ç†å‘˜ï¼ˆData Processorï¼‰ã€‚

ä½ çš„èŒè´£ï¼š
1. æ¥æ”¶åŸå§‹æ•°æ®å¹¶è¿›è¡Œæ¸…æ´—
2. è½¬æ¢æ•°æ®ä¸ºæŒ‡å®šæ ¼å¼
3. è¿”å›å¤„ç†åçš„æ•°æ®

å·¥ä½œæµç¨‹ï¼š
1. ä½¿ç”¨ clean_data å·¥å…·æ¸…æ´—åŸå§‹æ•°æ®
2. ä½¿ç”¨ transform_data å·¥å…·è½¬æ¢æ•°æ®æ ¼å¼
3. è¿”å›æœ€ç»ˆå¤„ç†ç»“æœ

é‡è¦ï¼š
- å¿…é¡»å…ˆæ¸…æ´—æ•°æ®ï¼Œå†è½¬æ¢æ ¼å¼
- å¤„ç†å®Œæˆåä½¿ç”¨ Action: finish è¿”å›ç»“æœ
"""

    return Agent(
        llm=llm,
        tools=tools,
        name="DataProcessor",
        system_prompt=system_prompt,
        max_iterations=5,
    )


def create_paper_finder(api_key: str) -> Agent:
    """
    åˆ›å»ºè®ºæ–‡æŸ¥æ‰¾å‘˜ (L3)

    èŒè´£ï¼š
    - æœç´¢ç›¸å…³å­¦æœ¯è®ºæ–‡
    - ä¸‹è½½è®ºæ–‡å…¨æ–‡
    """
    llm = DeepSeekLLM(api_key=api_key, model="deepseek-chat")

    tools_dict = create_all_tools()
    tools = [tools_dict["search_papers"], tools_dict["download_paper"]]

    system_prompt = """ä½ æ˜¯è®ºæ–‡æŸ¥æ‰¾å‘˜ï¼ˆPaper Finderï¼‰ã€‚

ä½ çš„èŒè´£ï¼š
1. æ ¹æ®å…³é”®è¯æœç´¢å­¦æœ¯è®ºæ–‡
2. ä¸‹è½½ç›¸å…³è®ºæ–‡çš„å…¨æ–‡
3. è¿”å›è®ºæ–‡ä¿¡æ¯å’Œå†…å®¹

å·¥ä½œæµç¨‹ï¼š
1. ä½¿ç”¨ search_papers æœç´¢ç›¸å…³è®ºæ–‡
2. é€‰æ‹©æœ€ç›¸å…³çš„è®ºæ–‡
3. ä½¿ç”¨ download_paper ä¸‹è½½è®ºæ–‡å…¨æ–‡
4. æ•´ç†å¹¶è¿”å›è®ºæ–‡ä¿¡æ¯å’Œå†…å®¹

é‡è¦ï¼š
- å…ˆæœç´¢è®ºæ–‡ï¼Œå†ä¸‹è½½æœ€ç›¸å…³çš„å‡ ç¯‡
- å®Œæˆåä½¿ç”¨ Action: finish è¿”å›ç»“æœ
"""

    return Agent(
        llm=llm,
        tools=tools,
        name="PaperFinder",
        system_prompt=system_prompt,
        max_iterations=5,
    )


def create_summary_generator(api_key: str) -> Agent:
    """
    åˆ›å»ºæ‘˜è¦ç”Ÿæˆå‘˜ (L3)

    èŒè´£ï¼š
    - ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
    - æå–å…³é”®å‘ç°
    """
    llm = DeepSeekLLM(api_key=api_key, model="deepseek-chat")

    tools_dict = create_all_tools()
    tools = [tools_dict["summarize_text"], tools_dict["extract_key_findings"]]

    system_prompt = """ä½ æ˜¯æ‘˜è¦ç”Ÿæˆå‘˜ï¼ˆSummary Generatorï¼‰ã€‚

ä½ çš„èŒè´£ï¼š
1. ä¸ºç»™å®šæ–‡æœ¬ç”Ÿæˆç®€æ´æ‘˜è¦
2. æå–å…³é”®å‘ç°å’Œè¦ç‚¹
3. è¿”å›ç»“æ„åŒ–çš„æ‘˜è¦ç»“æœ

å·¥ä½œæµç¨‹ï¼š
1. ä½¿ç”¨ summarize_text ç”Ÿæˆæ•´ä½“æ‘˜è¦
2. ä½¿ç”¨ extract_key_findings æå–å…³é”®å‘ç°
3. æ•´åˆæ‰€æœ‰ä¿¡æ¯è¿”å›å®Œæ•´æ‘˜è¦

é‡è¦ï¼š
- æ‘˜è¦è¦ç®€æ´ä½†åŒ…å«å…³é”®ä¿¡æ¯
- å…³é”®å‘ç°åº”è¯¥ç»“æ„æ¸…æ™°
- å®Œæˆåä½¿ç”¨ Action: finish è¿”å›ç»“æœ
"""

    return Agent(
        llm=llm,
        tools=tools,
        name="SummaryGenerator",
        system_prompt=system_prompt,
        max_iterations=5,
    )


# ============================================================================
# Layer 2 Agents (ä¸­å±‚ Agents) - åè°ƒåº•å±‚ agents
# ============================================================================


def create_data_analyst(api_key: str) -> Agent:
    """
    åˆ›å»ºæ•°æ®åˆ†æå¸ˆ (L2)

    èŒè´£ï¼š
    - åè°ƒæ•°æ®é‡‡é›†å’Œå¤„ç†å·¥ä½œ
    - ç®¡ç† DataCollector å’Œ DataProcessor
    - æä¾›æ•°æ®åˆ†æç»“æœ
    """
    llm = DeepSeekLLM(api_key=api_key, model="deepseek-chat")

    # åˆ›å»ºå­ agents
    data_collector = create_data_collector(api_key)
    data_processor = create_data_processor(api_key)

    system_prompt = """ä½ æ˜¯æ•°æ®åˆ†æå¸ˆï¼ˆData Analystï¼‰ã€‚

ä½ æœ‰ä¸¤ä¸ªåŠ©æ‰‹ï¼š
1. DataCollector - è´Ÿè´£æ”¶é›†åŸå§‹æ•°æ®
2. DataProcessor - è´Ÿè´£æ¸…æ´—å’Œè½¬æ¢æ•°æ®

ä½ çš„èŒè´£ï¼š
1. æ¥æ”¶æ•°æ®åˆ†æä»»åŠ¡
2. å°†ä»»åŠ¡åˆ†è§£ä¸ºæ•°æ®æ”¶é›†å’Œå¤„ç†ä¸¤ä¸ªæ­¥éª¤
3. å…ˆå¯åŠ¨ DataCollector æ”¶é›†æ•°æ®
4. ç­‰å¾… DataCollector å®Œæˆ
5. å°†æ”¶é›†åˆ°çš„æ•°æ®äº¤ç»™ DataProcessor å¤„ç†
6. ç­‰å¾… DataProcessor å®Œæˆ
7. æ•´åˆç»“æœå¹¶è¿”å›æœ€ç»ˆçš„æ•°æ®åˆ†ææŠ¥å‘Š

é‡è¦æ­¥éª¤ï¼š
1. ä½¿ç”¨ launch_subagents å¯åŠ¨ DataCollector
   æ ¼å¼ï¼šAction: launch_subagents
        Agents: ["DataCollector"]
        Tasks: ["æ”¶é›†å…³äºXçš„æ•°æ®"]

2. ä½¿ç”¨ wait_for_subagents ç­‰å¾…å®Œæˆ

3. æ”¶åˆ° DataCollector ç»“æœåï¼š
   - æ£€æŸ¥çŠ¶æ€ï¼Œå¦‚æœæœ‰ ğŸ”„ è¿è¡Œä¸­çš„ï¼Œç»§ç»­ wait
   - å¦‚æœç³»ç»Ÿæç¤º"æ‰€æœ‰å­ Agent éƒ½å·²å®Œæˆ"ï¼Œå¯åŠ¨ DataProcessor

4. å¯åŠ¨ DataProcessorï¼š
   æ ¼å¼ï¼šAction: launch_subagents
        Agents: ["DataProcessor"]
        Tasks: ["å¤„ç†ä»¥ä¸‹æ•°æ®ï¼š{DataCollectorçš„ç»“æœ}"]

5. å†æ¬¡ wait_for_subagents ç­‰å¾…å¤„ç†å®Œæˆ

6. æ”¶åˆ° DataProcessor ç»“æœåï¼š
   - å¦‚æœç³»ç»Ÿæç¤º"æ‰€æœ‰å­ Agent éƒ½å·²å®Œæˆ"ï¼Œæ•´åˆç»“æœ
   - ä½¿ç”¨ Action: finish è¿”å›æ•°æ®åˆ†ææŠ¥å‘Š

æ³¨æ„ï¼š
- å¿…é¡»ç­‰å¾…ä¸€ä¸ªå­ agent å®Œæˆæ‰èƒ½å¯åŠ¨ä¸‹ä¸€ä¸ª
- æ ¹æ®ç³»ç»Ÿçš„çŠ¶æ€æç¤ºåˆ¤æ–­æ˜¯å¦æ‰€æœ‰å­ Agent éƒ½å®Œæˆ
- åªæœ‰å½“ç³»ç»Ÿæ˜ç¡®æç¤º"æ‰€æœ‰å­ Agent éƒ½å·²å®Œæˆ"æ—¶æ‰èƒ½ finish
"""

    return Agent(
        llm=llm,
        subagents={
            "DataCollector": data_collector,
            "DataProcessor": data_processor,
        },
        name="DataAnalyst",
        system_prompt=system_prompt,
        max_iterations=15,
    )


def create_literature_researcher(api_key: str) -> Agent:
    """
    åˆ›å»ºæ–‡çŒ®ç ”ç©¶å‘˜ (L2)

    èŒè´£ï¼š
    - åè°ƒè®ºæ–‡æŸ¥æ‰¾å’Œæ‘˜è¦ç”Ÿæˆ
    - ç®¡ç† PaperFinder å’Œ SummaryGenerator
    - æä¾›æ–‡çŒ®ç»¼è¿°
    """
    llm = DeepSeekLLM(api_key=api_key, model="deepseek-chat")

    # åˆ›å»ºå­ agents
    paper_finder = create_paper_finder(api_key)
    summary_generator = create_summary_generator(api_key)

    system_prompt = """ä½ æ˜¯æ–‡çŒ®ç ”ç©¶å‘˜ï¼ˆLiterature Researcherï¼‰ã€‚

ä½ æœ‰ä¸¤ä¸ªåŠ©æ‰‹ï¼š
1. PaperFinder - è´Ÿè´£æŸ¥æ‰¾å’Œä¸‹è½½å­¦æœ¯è®ºæ–‡
2. SummaryGenerator - è´Ÿè´£ç”Ÿæˆæ‘˜è¦å’Œæå–å…³é”®å‘ç°

ä½ çš„èŒè´£ï¼š
1. æ¥æ”¶æ–‡çŒ®ç ”ç©¶ä»»åŠ¡
2. å…ˆå¯åŠ¨ PaperFinder æŸ¥æ‰¾ç›¸å…³è®ºæ–‡
3. ç­‰å¾… PaperFinder å®Œæˆ
4. å°†è®ºæ–‡å†…å®¹äº¤ç»™ SummaryGenerator ç”Ÿæˆæ‘˜è¦
5. ç­‰å¾… SummaryGenerator å®Œæˆ
6. æ•´åˆæ‰€æœ‰ç»“æœï¼Œè¿”å›å®Œæ•´çš„æ–‡çŒ®ç»¼è¿°

é‡è¦æ­¥éª¤ï¼š
1. ä½¿ç”¨ launch_subagents å¯åŠ¨ PaperFinder
   æ ¼å¼ï¼šAction: launch_subagents
        Agents: ["PaperFinder"]
        Tasks: ["æŸ¥æ‰¾å…³äºXçš„è®ºæ–‡"]

2. ä½¿ç”¨ wait_for_subagents ç­‰å¾…å®Œæˆ

3. æ”¶åˆ° PaperFinder ç»“æœåï¼š
   - æ£€æŸ¥çŠ¶æ€ï¼Œå¦‚æœæœ‰ ğŸ”„ è¿è¡Œä¸­çš„ï¼Œç»§ç»­ wait
   - å¦‚æœç³»ç»Ÿæç¤º"æ‰€æœ‰å­ Agent éƒ½å·²å®Œæˆ"ï¼Œå¯åŠ¨ SummaryGenerator

4. å¯åŠ¨ SummaryGeneratorï¼š
   æ ¼å¼ï¼šAction: launch_subagents
        Agents: ["SummaryGenerator"]
        Tasks: ["ç”Ÿæˆä»¥ä¸‹å†…å®¹çš„æ‘˜è¦ï¼š{PaperFinderçš„ç»“æœ}"]

5. å†æ¬¡ wait_for_subagents ç­‰å¾…å®Œæˆ

6. æ”¶åˆ° SummaryGenerator ç»“æœåï¼š
   - å¦‚æœç³»ç»Ÿæç¤º"æ‰€æœ‰å­ Agent éƒ½å·²å®Œæˆ"ï¼Œæ•´åˆç»“æœ
   - ä½¿ç”¨ Action: finish è¿”å›æ–‡çŒ®ç»¼è¿°

æ³¨æ„ï¼š
- å¿…é¡»æŒ‰é¡ºåºæ‰§è¡Œï¼šå…ˆæ‰¾è®ºæ–‡ï¼Œå†ç”Ÿæˆæ‘˜è¦
- æ¯æ¬¡å¯åŠ¨å­ agent åéƒ½è¦ wait
- æ ¹æ®ç³»ç»Ÿçš„çŠ¶æ€æç¤ºåˆ¤æ–­æ˜¯å¦æ‰€æœ‰å­ Agent éƒ½å®Œæˆ
- åªæœ‰å½“ç³»ç»Ÿæ˜ç¡®æç¤º"æ‰€æœ‰å­ Agent éƒ½å·²å®Œæˆ"æ—¶æ‰èƒ½ finish
"""

    return Agent(
        llm=llm,
        subagents={
            "PaperFinder": paper_finder,
            "SummaryGenerator": summary_generator,
        },
        name="LiteratureResearcher",
        system_prompt=system_prompt,
        max_iterations=15,
    )


# ============================================================================
# Layer 1 Agent (é¡¶å±‚ Agent) - ç ”ç©¶æ€»ç›‘
# ============================================================================


def create_research_director(api_key: str) -> Agent:
    """
    åˆ›å»ºç ”ç©¶æ€»ç›‘ (L1)

    èŒè´£ï¼š
    - æ¥æ”¶ç ”ç©¶ä»»åŠ¡
    - åè°ƒæ•°æ®åˆ†æå’Œæ–‡çŒ®ç ”ç©¶
    - ç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š
    """
    llm = DeepSeekLLM(api_key=api_key, model="deepseek-chat")

    # åˆ›å»ºå­ agents
    data_analyst = create_data_analyst(api_key)
    literature_researcher = create_literature_researcher(api_key)

    system_prompt = """ä½ æ˜¯ç ”ç©¶æ€»ç›‘ï¼ˆResearch Directorï¼‰ã€‚

ä½ æœ‰ä¸¤ä¸ªæ ¸å¿ƒå›¢é˜Ÿï¼š
1. DataAnalyst - æ•°æ®åˆ†æå›¢é˜Ÿï¼ˆåŒ…å« DataCollector å’Œ DataProcessorï¼‰
2. LiteratureResearcher - æ–‡çŒ®ç ”ç©¶å›¢é˜Ÿï¼ˆåŒ…å« PaperFinder å’Œ SummaryGeneratorï¼‰

ä½ çš„èŒè´£ï¼š
1. æ¥æ”¶ç ”ç©¶ä»»åŠ¡
2. å°†ä»»åŠ¡åˆ†è§£ä¸ºæ•°æ®åˆ†æå’Œæ–‡çŒ®ç ”ç©¶ä¸¤éƒ¨åˆ†
3. åŒæ—¶å¯åŠ¨ä¸¤ä¸ªå›¢é˜Ÿå¹¶è¡Œå·¥ä½œ
4. ç­‰å¾…ä¸¤ä¸ªå›¢é˜Ÿå®Œæˆ
5. æ•´åˆæ‰€æœ‰ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š

å·¥ä½œæµç¨‹ï¼š
1. åˆ†æç ”ç©¶ä»»åŠ¡ï¼Œç¡®å®šæ•°æ®åˆ†æéœ€æ±‚å’Œæ–‡çŒ®ç ”ç©¶éœ€æ±‚

2. åŒæ—¶å¯åŠ¨ä¸¤ä¸ªå›¢é˜Ÿï¼š
   Action: launch_subagents
   Agents: ["DataAnalyst", "LiteratureResearcher"]
   Tasks: ["æ•°æ®åˆ†æä»»åŠ¡æè¿°", "æ–‡çŒ®ç ”ç©¶ä»»åŠ¡æè¿°"]

3. ç­‰å¾…å›¢é˜Ÿå®Œæˆï¼š
   Action: wait_for_subagents

4. å½“æ”¶åˆ°å­ Agent å®Œæˆé€šçŸ¥æ—¶ï¼š
   - ç³»ç»Ÿä¼šæ˜¾ç¤ºå½“å‰çŠ¶æ€ï¼ˆå“ªäº›å·²å®Œæˆï¼Œå“ªäº›è¿˜åœ¨è¿è¡Œï¼‰
   - å¦‚æœè¿˜æœ‰å­ Agent åœ¨è¿è¡Œï¼ˆçŠ¶æ€æ˜¾ç¤º ğŸ”„ è¿è¡Œä¸­ï¼‰ï¼Œç»§ç»­ wait_for_subagents
   - å¦‚æœæ‰€æœ‰å­ Agent éƒ½å·²å®Œæˆï¼ˆæ²¡æœ‰ ğŸ”„ è¿è¡Œä¸­çš„ï¼‰ï¼Œç«‹å³è¿›å…¥æ­¥éª¤ 5

5. æ‰€æœ‰å›¢é˜Ÿå®Œæˆåï¼ˆç³»ç»Ÿä¼šæ˜ç¡®æç¤º"æ‰€æœ‰å­ Agent éƒ½å·²å®Œæˆ"ï¼‰ï¼š
   - æ•´åˆä¸¤ä¸ªå›¢é˜Ÿçš„ç»“æœ
   - ç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Šï¼ˆåŒ…æ‹¬æ•°æ®åˆ†æã€æ–‡çŒ®ç»¼è¿°ã€ç»“è®ºï¼‰
   - ä½¿ç”¨ Action: finish è¿”å›æœ€ç»ˆæŠ¥å‘Š

é‡è¦æ³¨æ„äº‹é¡¹ï¼š
- ä¸¤ä¸ªå›¢é˜Ÿåº”è¯¥å¹¶è¡Œå·¥ä½œï¼Œä¸æ˜¯é¡ºåºæ‰§è¡Œ
- æ¯ä¸ªå›¢é˜Ÿå®Œæˆåéƒ½ä¼šæ”¶åˆ°é€šçŸ¥å¹¶æ›´æ–°çŠ¶æ€
- å½“ç³»ç»Ÿæç¤º"æ‰€æœ‰å­ Agent éƒ½å·²å®Œæˆ"æ—¶ï¼Œä¸è¦å† wait_for_subagents
- å¿…é¡»æ ¹æ®çŠ¶æ€æç¤ºåˆ¤æ–­æ˜¯å¦è¿˜éœ€è¦ç­‰å¾…
- åªæœ‰å½“çŠ¶æ€ä¸­æ²¡æœ‰ ğŸ”„ è¿è¡Œä¸­çš„å­ Agent æ—¶æ‰èƒ½ finish
"""

    return Agent(
        llm=llm,
        subagents={
            "DataAnalyst": data_analyst,
            "LiteratureResearcher": literature_researcher,
        },
        name="ResearchDirector",
        system_prompt=system_prompt,
        max_iterations=20,
    )


# ============================================================================
# Main Function
# ============================================================================


async def main():
    """è¿è¡Œä¸‰å±‚ agent ç³»ç»Ÿç¤ºä¾‹"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_env()

    api_key = get_deepseek_api_key()
    if not api_key:
        print("é”™è¯¯ï¼šè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DEEPSEEK_API_KEY")
        return

    print("=" * 80)
    print("ä¸‰å±‚ Agent ç³»ç»Ÿç¤ºä¾‹ - ç ”ç©¶åŠ©ç†ç³»ç»Ÿ")
    print("=" * 80)
    print()
    print("ç³»ç»Ÿæ¶æ„ï¼š")
    print("  L1: ResearchDirector (ç ”ç©¶æ€»ç›‘)")
    print("      â”œâ”€â”€ L2: DataAnalyst (æ•°æ®åˆ†æå¸ˆ)")
    print("      â”‚   â”œâ”€â”€ L3: DataCollector (æ•°æ®é‡‡é›†å‘˜)")
    print("      â”‚   â””â”€â”€ L3: DataProcessor (æ•°æ®å¤„ç†å‘˜)")
    print("      â””â”€â”€ L2: LiteratureResearcher (æ–‡çŒ®ç ”ç©¶å‘˜)")
    print("          â”œâ”€â”€ L3: PaperFinder (è®ºæ–‡æŸ¥æ‰¾å‘˜)")
    print("          â””â”€â”€ L3: SummaryGenerator (æ‘˜è¦ç”Ÿæˆå‘˜)")
    print()
    print("ç‰¹ç‚¹ï¼š")
    print("  âœ“ ä¸‰å±‚åµŒå¥—è°ƒç”¨")
    print("  âœ“ L2 agents å¹¶è¡Œæ‰§è¡Œ")
    print("  âœ“ æ¯å±‚ agent éƒ½å¯ä»¥å¤šè½®è°ƒç”¨å…¶å­ agents")
    print("  âœ“ é€’å½’ä»»åŠ¡åˆ†è§£å’Œç»“æœèšåˆ")
    print("=" * 80)
    print()

    # åˆå§‹åŒ–å¼‚æ­¥æ—¥å¿—
    logger = await init_logger(log_dir="logs", console_output=True)

    try:
        # åˆ›å»ºç ”ç©¶æ€»ç›‘
        print("æ­£åœ¨åˆå§‹åŒ–ç ”ç©¶æ€»ç›‘ç³»ç»Ÿ...")
        research_director = create_research_director(api_key)
        print("âœ“ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print()

        # å®šä¹‰ç ”ç©¶ä»»åŠ¡
        research_task = """
è¯·å¯¹"äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—å¥åº·é¢†åŸŸçš„åº”ç”¨"è¿›è¡Œå…¨é¢ç ”ç©¶ã€‚

ç ”ç©¶è¦æ±‚ï¼š
1. æ•°æ®åˆ†æéƒ¨åˆ†ï¼š
   - æ”¶é›†åŒ»ç–—å¥åº·ç›¸å…³çš„ç»Ÿè®¡æ•°æ®
   - æ”¶é›† AI åº”ç”¨æ¡ˆä¾‹çš„æ•°æ®
   - æ¸…æ´—å’Œåˆ†æè¿™äº›æ•°æ®

2. æ–‡çŒ®ç ”ç©¶éƒ¨åˆ†ï¼š
   - æŸ¥æ‰¾ AI åŒ»ç–—ç›¸å…³çš„æœ€æ–°å­¦æœ¯è®ºæ–‡
   - ç”Ÿæˆæ–‡çŒ®ç»¼è¿°å’Œå…³é”®å‘ç°

æœ€ç»ˆéœ€è¦ä¸€ä»½æ•´åˆæ•°æ®åˆ†æå’Œæ–‡çŒ®ç ”ç©¶çš„å®Œæ•´æŠ¥å‘Šã€‚
"""

        print("ç ”ç©¶ä»»åŠ¡ï¼š")
        print(research_task)
        print()
        print("å¼€å§‹æ‰§è¡Œç ”ç©¶ä»»åŠ¡...")
        print("=" * 80)
        print()

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ‰§è¡Œä»»åŠ¡
        result = await research_director._run_async(task=research_task)

        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        elapsed_time = end_time - start_time

        # è¾“å‡ºç»“æœ
        print()
        print("=" * 80)
        print("ç ”ç©¶å®Œæˆï¼")
        print("=" * 80)
        print()
        print(f"æ‰§è¡ŒçŠ¶æ€: {'âœ“ æˆåŠŸ' if result.success else 'âœ— å¤±è´¥'}")
        print(f"è¿­ä»£æ¬¡æ•°: {result.iterations}")
        print(f"æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        print()
        print("æœ€ç»ˆç ”ç©¶æŠ¥å‘Šï¼š")
        print("-" * 80)
        print(result.content)
        print("-" * 80)
        print()

        # ç³»ç»Ÿç»Ÿè®¡
        print("ç³»ç»Ÿæ‰§è¡Œç»Ÿè®¡ï¼š")
        print(f"  â€¢ æ¶‰åŠ agents: 7 ä¸ª (1ä¸ªL1 + 2ä¸ªL2 + 4ä¸ªL3)")
        print(f"  â€¢ ä½¿ç”¨ tools: 8 ä¸ª")
        print(f"  â€¢ å¹¶è¡Œæ‰§è¡Œ: L2 å±‚çš„ 2 ä¸ª agents å¹¶è¡Œ")
        print(f"  â€¢ åµŒå¥—æ·±åº¦: 3 å±‚")
        print()
        print(f"æ—¥å¿—ä¿å­˜ä½ç½®: logs/")
        print()

    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        import traceback

        traceback.print_exc()
    finally:
        # å…³é—­æ—¥å¿—
        await close_logger()


if __name__ == "__main__":
    asyncio.run(main())
