"""
Real-time reporting example with parallel subagents.

This example demonstrates:
1. Parent agent launches two subagents in parallel (WeatherAgent and StockAgent)
2. WeatherAgent fetches weather data (takes 3 seconds)
3. StockAgent fetches stock price (takes 10 seconds)
4. **KEY FEATURE**: Parent agent reports each result IMMEDIATELY when available,
   rather than waiting for all subagents to finish
5. This shows real-time streaming behavior where faster results are shown first

Expected behavior:
- Both agents start at t=0
- WeatherAgent finishes at ~3s -> Parent immediately reports weather
- StockAgent finishes at ~10s -> Parent immediately reports stock price
- Parent then provides final summary

Usage:
    # Use Copilot (default)
    python examples/async_parallel_agents_real.py

    # Use DeepSeek
    python examples/async_parallel_agents_real.py --llm deepseek
    # or
    LLM_PROVIDER=deepseek python examples/async_parallel_agents_real.py
"""

import asyncio
import time
import os
import sys
from agent.agent import Agent
from agent.copilot_llm import CopilotLLM
from agent.deepseek_llm import DeepSeekLLM
from agent.tool import Tool
from agent.async_logger import init_logger, close_logger
from agent.config import load_env, get_deepseek_api_key


# ============================================================================
# LLM Provider Configuration
# ============================================================================


def get_llm_provider():
    """
    Get LLM provider from environment variable or command line.

    Priority:
    1. Command line argument: --llm copilot/deepseek
    2. Environment variable: LLM_PROVIDER=copilot/deepseek
    3. Default: copilot

    Returns:
        str: "copilot" or "deepseek"
    """
    # Check command line arguments
    if "--llm" in sys.argv:
        idx = sys.argv.index("--llm")
        if idx + 1 < len(sys.argv):
            provider = sys.argv[idx + 1].lower()
            if provider in ["copilot", "deepseek"]:
                return provider

    # Check environment variable
    provider = os.environ.get("LLM_PROVIDER", "copilot").lower()
    if provider in ["copilot", "deepseek"]:
        return provider

    # Default
    return "copilot"


def create_llm(provider: str | None = None):
    """
    Create LLM instance based on provider.

    Args:
        provider: "copilot" or "deepseek" (auto-detect if None)

    Returns:
        LLM instance
    """
    if provider is None:
        provider = get_llm_provider()

    if provider == "deepseek":
        api_key = get_deepseek_api_key()
        if not api_key:
            raise ValueError(
                "DEEPSEEK_API_KEY not found. Please set it in .env file or environment."
            )
        print(f"ğŸ¤– Using DeepSeek LLM (model: deepseek-chat)")
        return DeepSeekLLM(api_key=api_key, model="deepseek-chat")
    else:  # copilot
        print(f"ğŸ¤– Using GitHub Copilot LLM (model: claude-haiku-4.5)")
        return CopilotLLM(model="claude-haiku-4.5", temperature=0.7)


# ============================================================================
# Tools and Agents
# ============================================================================


def create_weather_tool() -> Tool:
    """Create a tool that fetches weather data (simulated with 3s delay)"""

    def get_weather(city: str) -> str:
        """
        Get weather information for a city.

        Args:
            city: City name (e.g., "Beijing", "Shanghai")

        Returns:
            Weather information
        """
        time.sleep(3)  # Simulate API call delay

        # Simulate weather data
        weather_data = {
            "Beijing": "æ™´å¤©ï¼Œæ°”æ¸© 15Â°Cï¼Œä¸œåŒ—é£ 3çº§",
            "Shanghai": "å¤šäº‘ï¼Œæ°”æ¸© 18Â°Cï¼Œä¸œå—é£ 2çº§",
            "Shenzhen": "å°é›¨ï¼Œæ°”æ¸© 22Â°Cï¼Œå—é£ 4çº§",
        }

        result = weather_data.get(city, f"{city}ï¼šæ™´å¤©ï¼Œæ°”æ¸© 20Â°Cï¼Œå¾®é£")
        return f"âœ… å¤©æ°”æŸ¥è¯¢æˆåŠŸï¼š{result}"

    return Tool(get_weather)


def create_stock_tool() -> Tool:
    """Create a tool that fetches stock price (simulated with 10s delay)"""

    def get_stock_price(symbol: str) -> str:
        """
        Get current stock price for a symbol.

        Args:
            symbol: Stock symbol (e.g., "AAPL", "TSLA", "BABA")

        Returns:
            Stock price information
        """
        time.sleep(10)  # Simulate complex financial API call

        # Simulate stock data
        stock_data = {
            "AAPL": "è‹¹æœ(AAPL): $182.45 â†‘ +1.2%",
            "TSLA": "ç‰¹æ–¯æ‹‰(TSLA): $245.80 â†“ -0.8%",
            "BABA": "é˜¿é‡Œå·´å·´(BABA): $88.90 â†‘ +2.3%",
        }

        result = stock_data.get(symbol, f"{symbol}: $100.00 â†” 0.0%")
        return f"âœ… è‚¡ç¥¨æŸ¥è¯¢æˆåŠŸï¼š{result}"

    return Tool(get_stock_price)


def create_weather_agent(llm) -> Agent:
    """
    Create weather agent that fetches weather data for Beijing.

    This agent will:
    1. Use get_weather tool to fetch weather for Beijing
    2. Takes ~3 seconds to complete
    3. Return weather information

    Args:
        llm: LLM instance (independent from other agents)
    """
    weather_tool = create_weather_tool()

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªå¤©æ°”æŸ¥è¯¢Agentã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. ä½¿ç”¨ get_weather å·¥å…·æŸ¥è¯¢åŒ—äº¬(Beijing)çš„å¤©æ°”
2. è¿”å›å¤©æ°”ä¿¡æ¯

é‡è¦ï¼šå¿…é¡»å…ˆè°ƒç”¨ get_weather(city="Beijing")ï¼Œç„¶åå°†ç»“æœè¿”å›ç»™ç”¨æˆ·ã€‚"""

    agent = Agent(
        llm=llm,
        tools=[weather_tool],
        name="WeatherAgent",
        system_prompt=system_prompt,
        max_iterations=5,
    )

    return agent


def create_stock_agent(llm) -> Agent:
    """
    Create stock agent that fetches stock price for AAPL.

    This agent will:
    1. Use get_stock_price tool to fetch AAPL stock price
    2. Takes ~10 seconds to complete
    3. Return stock price information

    Args:
        llm: LLM instance (independent from other agents)
    """
    stock_tool = create_stock_tool()

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªè‚¡ç¥¨æŸ¥è¯¢Agentã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. ä½¿ç”¨ get_stock_price å·¥å…·æŸ¥è¯¢è‹¹æœå…¬å¸(AAPL)çš„è‚¡ç¥¨ä»·æ ¼
2. è¿”å›è‚¡ç¥¨ä¿¡æ¯

é‡è¦ï¼šå¿…é¡»å…ˆè°ƒç”¨ get_stock_price(symbol="AAPL")ï¼Œç„¶åå°†ç»“æœè¿”å›ç»™ç”¨æˆ·ã€‚"""

    agent = Agent(
        llm=llm,
        tools=[stock_tool],
        name="StockAgent",
        system_prompt=system_prompt,
        max_iterations=5,
    )

    return agent


def create_parent_agent() -> Agent:
    """
    Create parent agent that coordinates weather and stock queries.

    KEY BEHAVIOR:
    - Launches both agents in parallel
    - When WeatherAgent finishes (fast, ~3s), immediately reports weather to user
    - Continues waiting for StockAgent
    - When StockAgent finishes (slow, ~10s), immediately reports stock price
    - Finally provides a summary

    This demonstrates real-time streaming behavior where results are
    reported as soon as they're available, not all at once.

    IMPORTANT: Each agent (parent, weather, stock) gets its own independent LLM instance
    to prevent conversation history contamination.
    """
    parent_llm = create_llm()

    # Create independent LLM instances for each subagent
    # This prevents conversation history sharing between agents
    weather_llm = create_llm()
    stock_llm = create_llm()

    weather_agent = create_weather_agent(weather_llm)
    stock_agent = create_stock_agent(stock_llm)

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æŸ¥è¯¢åè°ƒAgentã€‚

ä½ æœ‰ä¸¤ä¸ªå­Agentï¼š
1. WeatherAgent - æŸ¥è¯¢åŒ—äº¬å¤©æ°”ï¼ˆå¿«é€Ÿï¼Œçº¦3ç§’ï¼‰
2. StockAgent - æŸ¥è¯¢AAPLè‚¡ç¥¨ä»·æ ¼ï¼ˆæ…¢é€Ÿï¼Œçº¦10ç§’ï¼‰

**å…³é”®ä»»åŠ¡æµç¨‹ï¼ˆå®æ—¶æ±‡æŠ¥æ¨¡å¼ï¼‰**ï¼š

1. é¦–æ¬¡å¯åŠ¨ï¼šä½¿ç”¨ launch_subagents åŒæ—¶å¯åŠ¨ä¸¤ä¸ªAgent
   æ ¼å¼ï¼š
   Action: launch_subagents
   Agents: ["WeatherAgent", "StockAgent"]
   Tasks: ["æŸ¥è¯¢åŒ—äº¬å¤©æ°”", "æŸ¥è¯¢AAPLè‚¡ç¥¨ä»·æ ¼"]

2. ç­‰å¾…ç»“æœï¼šä½¿ç”¨ wait_for_subagents ç­‰å¾…

3. **æ¯æ¬¡è¢«å”¤é†’æ—¶ï¼ˆæœ‰Agentå®Œæˆï¼‰**ï¼š
   - åœ¨Thoughtä¸­å‘ç”¨æˆ·å®æ—¶æ±‡æŠ¥åˆšå®Œæˆçš„Agentç»“æœ
   - æ£€æŸ¥æ˜¯å¦è¿˜æœ‰pendingçš„Agent
   - å¦‚æœè¿˜æœ‰pendingçš„ï¼Œç»§ç»­ wait_for_subagents
   - å¦‚æœæ‰€æœ‰éƒ½å®Œæˆäº†ï¼Œä½¿ç”¨ finish æä¾›æœ€ç»ˆæ€»ç»“

**å®æ—¶æ±‡æŠ¥ç­–ç•¥**ï¼š
- ç”¨Thoughtå­—æ®µå®æ—¶å‘ç”¨æˆ·æ±‡æŠ¥æ¯ä¸ªAgentçš„å®Œæˆæƒ…å†µ
- ä¾‹å¦‚ï¼š
  * ç¬¬ä¸€æ¬¡è¢«å”¤é†’ï¼ˆWeatherAgentå®Œæˆï¼‰ï¼š
    Thought: "ã€å®æ—¶æ±‡æŠ¥ã€‘å¤©æ°”æŸ¥è¯¢å®Œæˆï¼åŒ—äº¬ï¼šæ™´å¤©ï¼Œ15Â°Cã€‚è‚¡ç¥¨æŸ¥è¯¢è¿˜åœ¨è¿›è¡Œä¸­..."
    Action: wait_for_subagents (ç»§ç»­ç­‰å¾…StockAgent)
  
  * ç¬¬äºŒæ¬¡è¢«å”¤é†’ï¼ˆStockAgentå®Œæˆï¼‰ï¼š
    Thought: "ã€å®æ—¶æ±‡æŠ¥ã€‘è‚¡ç¥¨æŸ¥è¯¢å®Œæˆï¼AAPL: $182.45 â†‘+1.2%ã€‚æ‰€æœ‰æŸ¥è¯¢å·²å®Œæˆã€‚"
    Action: finish (æä¾›æœ€ç»ˆæ€»ç»“)

**é‡è¦**ï¼š
- æ¯æ¬¡resumeæ—¶ï¼Œç«‹å³åœ¨Thoughtä¸­æ±‡æŠ¥æ–°å®Œæˆçš„ç»“æœ
- Thoughtæ˜¯ç”¨æˆ·å¯è§çš„ï¼Œç”¨å®ƒæ¥å®ç°å®æ—¶æ±‡æŠ¥
- ä¸è¦ç­‰æ‰€æœ‰Agentéƒ½å®Œæˆå†ä¸€èµ·æ±‡æŠ¥
- æœ€åfinishæ—¶åªéœ€è¦ç®€çŸ­æ€»ç»“å³å¯
"""

    agent = Agent(
        parent_llm,
        subagents={
            "WeatherAgent": weather_agent,
            "StockAgent": stock_agent,
        },
        name="ParentAgent",
        system_prompt=system_prompt,
        max_iterations=20,
    )

    return agent


async def main():
    """Run the real-time reporting example"""
    # Load environment variables
    load_env()

    # Get LLM provider
    provider = get_llm_provider()

    # Initialize async logger
    print("=" * 70)
    print(f"Real-Time Reporting Example with Parallel Subagents ({provider.upper()})")
    print("=" * 70)
    print()
    print("Scenario:")
    print("  - Query Beijing weather (fast, ~3s)")
    print("  - Query AAPL stock price (slow, ~10s)")
    print()
    print("Expected behavior:")
    print("  - Both queries start at t=0 (parallel execution)")
    print("  - Weather result reported at ~3s (as soon as available)")
    print("  - Stock result reported at ~10s (as soon as available)")
    print("  - NOT waiting for all results before reporting")
    print()
    print("This demonstrates real-time streaming of results!")
    print("=" * 70)
    print()

    logger = await init_logger(log_dir="logs", console_output=True)

    try:
        # Create parent agent
        parent_agent = create_parent_agent()

        # Run the parent agent
        start_time = time.time()

        result = await parent_agent._run_async(
            task="è¯·åŒæ—¶æŸ¥è¯¢åŒ—äº¬å¤©æ°”å’ŒAAPLè‚¡ç¥¨ä»·æ ¼ã€‚æ¯è·å¾—ä¸€ä¸ªç»“æœå°±ç«‹å³å‘æˆ‘æ±‡æŠ¥ï¼Œä¸è¦ç­‰æ‰€æœ‰ç»“æœéƒ½å‡ºæ¥å†ä¸€èµ·å‘Šè¯‰æˆ‘ã€‚",
        )

        end_time = time.time()
        elapsed = end_time - start_time

        # Print results
        print()
        print("=" * 70)
        print("Results")
        print("=" * 70)
        print(f"Success: {result.success}")
        print(f"Iterations: {result.iterations}")
        print(f"Total Time: {elapsed:.2f}s")
        print()
        print(f"Final Content:\n{result.content}")
        print()

        # Verify parallel execution
        if elapsed < 12:  # Should be ~10s (slower task), not 13s (sequential)
            print("âœ… Parallel execution confirmed!")
            print(f"   Weather query: ~3s")
            print(f"   Stock query: ~10s")
            print(f"   Total time: {elapsed:.1f}s (parallel)")
            print(f"   If sequential: would be ~13s")
        else:
            print("âš ï¸  Execution took longer than expected")
            print(f"   Total time: {elapsed:.1f}s")

        print()
        print(f"Logs saved to: logs/")

    finally:
        # Close logger
        await close_logger()


if __name__ == "__main__":
    asyncio.run(main())
